package types

import (
	"database/sql"
	"database/sql/driver"
	"encoding/json"
	"errors"
	"fmt"
	"strings"
	"time"

	"github.com/google/uuid"
	"github.com/mark3labs/mcp-go/mcp"

	openai "github.com/sashabaranov/go-openai"
	"gorm.io/datatypes"
)

type Interaction struct {
	ID string `json:"id" gorm:"primaryKey"`
	// GenerationID, starts at 0, increments for each regeneration (when user retries a message, anywhere from the past)
	// it is used to keep a timeline when querying the database for messages or viewing previous generations
	GenerationID int `json:"generation_id" gorm:"primaryKey"`

	Created   time.Time `json:"created"`
	Updated   time.Time `json:"updated"`
	Scheduled time.Time `json:"scheduled"`
	Completed time.Time `json:"completed"`
	// the ID of the runner that processed this interaction
	Runner string `json:"runner"` // e.g. 0

	Mode SessionMode `json:"mode"`

	AppID     string `json:"app_id" gorm:"index"`
	SessionID string `json:"session_id" gorm:"index"`
	UserID    string `json:"user_id" gorm:"index"`

	SystemPrompt string `json:"system_prompt"` // System prompt for the interaction (copy of the session's system prompt that was used to create this interaction)

	PromptMessage        string         `json:"prompt_message"`                                           // User prompt (text)
	PromptMessageContent MessageContent `json:"prompt_message_content" gorm:"type:jsonb;serializer:json"` // User prompt (multi-part)

	// TODO: add the full multi-part response content
	// ResponseMessageContent MessageContent `json:"response_message_content"` // LLM response
	ResponseMessage        string         `json:"response_message"`                                  // LLM response
	ResponseFormat         ResponseFormat `json:"response_format" gorm:"type:jsonb;serializer:json"` // e.g. json
	ResponseFormatResponse string         `json:"response_format_response"`                          // e.g. json

	DisplayMessage string           `json:"display_message"` // if this is defined, the UI will always display it instead of the message (so we can augment the internal prompt with RAG context)
	DurationMs     int              `json:"duration_ms"`     // How long the interaction took to complete in milliseconds
	State          InteractionState `json:"state"`
	Status         string           `json:"status"`
	Error          string           `json:"error"`

	Trigger string `json:"trigger"` // Session (default), slack, crisp, etc

	RagResults []*SessionRAGResult `json:"rag_results" gorm:"type:jsonb;serializer:json"`

	// Model function calling, not to be mistaken with Helix tools
	Tools []openai.Tool `json:"tools" gorm:"type:jsonb;serializer:json"`

	// This can be either a string or an ToolChoice object.
	// ToolChoice any `json:"tool_choice,omitempty" gorm:"type:jsonb;serializer:json"`

	// For Role=assistant prompts this may be set to the tool calls generated by the model, such as function calls.
	ToolCalls []openai.ToolCall `json:"tool_calls,omitempty" gorm:"type:jsonb;serializer:json"`

	// For Role=tool prompts this should be set to the ID given in the assistant's prior request to call a tool.
	ToolCallID string `json:"tool_call_id,omitempty"`

	Usage Usage `json:"usage" gorm:"type:jsonb;serializer:json"`

	Feedback        Feedback `json:"feedback" gorm:"index"`
	FeedbackMessage string   `json:"feedback_message"`
}

type FeedbackRequest struct {
	Feedback        Feedback `json:"feedback" gorm:"index"`
	FeedbackMessage string   `json:"feedback_message"`
}

type Feedback string

const (
	FeedbackLike    Feedback = "like"
	FeedbackDislike Feedback = "dislike"
)

func InteractionsToOpenAIMessages(systemPrompt string, interactions []*Interaction) []openai.ChatCompletionMessage {
	messages := []openai.ChatCompletionMessage{}

	if systemPrompt != "" {
		messages = append(messages, openai.ChatCompletionMessage{
			Role:    openai.ChatMessageRoleSystem,
			Content: systemPrompt,
		})
	}

	// Each interaction will have user and assistant messages
	for _, interaction := range interactions {
		switch interaction.State {
		case InteractionStateComplete:
			// Interaction contains both user and assistant messages
			messages = append(messages, interaction.ToOpenAIUserMessage())

			assistantMessage, ok := interaction.ToOpenAIAssistantMessage()
			if ok {
				messages = append(messages, assistantMessage)
			}
		case InteractionStateWaiting:
			// Interaction contains only user message
			messages = append(messages, interaction.ToOpenAIUserMessage())
		}
	}

	return messages
}

func (i *Interaction) ToOpenAISystemMessage() openai.ChatCompletionMessage {
	return openai.ChatCompletionMessage{
		Role:    openai.ChatMessageRoleSystem,
		Content: i.SystemPrompt,
	}
}

func (i *Interaction) ToOpenAIUserMessage() openai.ChatCompletionMessage {
	if len(i.PromptMessageContent.Parts) == 0 {
		return openai.ChatCompletionMessage{
			Role:    openai.ChatMessageRoleUser,
			Content: i.PromptMessage,
		}
	}

	return openai.ChatCompletionMessage{
		Role:         openai.ChatMessageRoleUser,
		MultiContent: i.GetMessageMultiContentPart(),
	}
}

func (i *Interaction) ToOpenAIAssistantMessage() (openai.ChatCompletionMessage, bool) {
	if i.ResponseMessage == "" {
		return openai.ChatCompletionMessage{}, false
	}

	return openai.ChatCompletionMessage{
		Role:    openai.ChatMessageRoleAssistant,
		Content: i.ResponseMessage,
	}, true
}

type ListInteractionsQuery struct {
	AppID         string
	SessionID     string
	InteractionID string
	UserID        string
	GenerationID  int // Use -1 to get all generations for a session
	Page          int
	PerPage       int
	Feedback      string
	Order         string // Defaults to ID ASC
}

// GetMessageMultiContentPart - probably specifically for PromptContent
func (i *Interaction) GetMessageMultiContentPart() []openai.ChatMessagePart {
	parts := []openai.ChatMessagePart{}

	// If content is empty, return one slice with text message
	if len(i.PromptMessageContent.Parts) == 0 {
		return []openai.ChatMessagePart{
			{Type: "text", Text: i.PromptMessage},
		}
	}

	for _, part := range i.PromptMessageContent.Parts {
		switch p := part.(type) {
		case string:
			parts = append(parts, openai.ChatMessagePart{Type: "text", Text: p})
		case TextPart:
			parts = append(parts, openai.ChatMessagePart{Type: "text", Text: p.Text})
		case ImageURLPart:
			parts = append(parts, openai.ChatMessagePart{Type: "image_url", ImageURL: &openai.ChatMessageImageURL{
				URL:    p.ImageURL.URL,
				Detail: openai.ImageURLDetail(p.ImageURL.Detail),
			}})
		case map[string]interface{}:
			if typeVal, typeOk := p["type"].(string); typeOk {
				switch typeVal {
				case "text":
					if textVal, textOk := p["text"].(string); textOk {
						parts = append(parts, openai.ChatMessagePart{Type: "text", Text: textVal})
					}
				case "image_url":
					if imageURLVal, imageURLOk := p["image_url"].(map[string]interface{}); imageURLOk {
						if urlVal, urlOk := imageURLVal["url"].(string); urlOk {
							detail := "auto"
							if detailVal, detailOk := imageURLVal["detail"].(string); detailOk {
								detail = detailVal
							}
							parts = append(parts, openai.ChatMessagePart{
								Type: "image_url",
								ImageURL: &openai.ChatMessageImageURL{
									URL:    urlVal,
									Detail: openai.ImageURLDetail(detail),
								},
							})
						}
					}
				}
			}
		}
	}
	return parts
}

type ResponseFormatType string

const (
	ResponseFormatTypeJSONObject ResponseFormatType = "json_object"
	ResponseFormatTypeText       ResponseFormatType = "text"
)

type ResponseFormat struct {
	Type       ResponseFormatType                             `json:"type"`
	JSONSchema *openai.ChatCompletionResponseFormatJSONSchema `json:"schema"`
}

type InteractionMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type TextSplitterType string

const (
	TextSplitterTypeMarkdown TextSplitterType = "markdown"
	TextSplitterTypeText     TextSplitterType = "text"
)

type RAGSettings struct {
	DistanceFunction string  `json:"distance_function" yaml:"distance_function"` // this is one of l2, inner_product or cosine - will default to cosine
	Threshold        float64 `json:"threshold" yaml:"threshold"`                 // this is the threshold for a "good" answer - will default to 0.2
	ResultsCount     int     `json:"results_count" yaml:"results_count"`         // this is the max number of results to return - will default to 3

	TextSplitter       TextSplitterType `json:"text_splitter" yaml:"text_splitter"`             // Markdown if empty or 'text'
	ChunkSize          int              `json:"chunk_size" yaml:"chunk_size"`                   // the size of each text chunk - will default to 2000 bytes
	ChunkOverflow      int              `json:"chunk_overflow" yaml:"chunk_overflow"`           // the amount of overlap between chunks - will default to 32 bytes
	DisableChunking    bool             `json:"disable_chunking" yaml:"disable_chunking"`       // if true, we will not chunk the text and send the entire file to the RAG indexing endpoint
	DisableDownloading bool             `json:"disable_downloading" yaml:"disable_downloading"` // if true, we will not download the file and send the URL to the RAG indexing endpoint
	PromptTemplate     string           `json:"prompt_template" yaml:"prompt_template"`         // the prompt template to use for the RAG query
	EnableVision       bool             `json:"enable_vision" yaml:"enable_vision"`             // if true, we will use the vision pipeline -- Future - might want to specify different pipelines

	// RAG endpoint configuration if used with a custom RAG service
	IndexURL  string `json:"index_url" yaml:"index_url"`   // the URL of the index endpoint (defaults to Helix RAG_INDEX_URL env var)
	QueryURL  string `json:"query_url" yaml:"query_url"`   // the URL of the query endpoint (defaults to Helix RAG_QUERY_URL env var)
	DeleteURL string `json:"delete_url" yaml:"delete_url"` // the URL of the delete endpoint (defaults to Helix RAG_DELETE_URL env var)

	Typesense struct {
		URL        string `json:"url" yaml:"url"`
		APIKey     string `json:"api_key" yaml:"api_key"`
		Collection string `json:"collection" yaml:"collection"`
	} `json:"typesense" yaml:"typesense"`
}

func (r RAGSettings) Value() (driver.Value, error) {
	j, err := json.Marshal(r)
	return j, err
}

func (r *RAGSettings) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New("type assertion .([]byte) failed")
	}
	var result RAGSettings
	if err := json.Unmarshal(source, &result); err != nil {
		return err
	}
	*r = result
	return nil
}

func (RAGSettings) GormDataType() string {
	return "json"
}

type RAGPipeline string

var (
	TextPipeline   RAGPipeline = "text_pipeline"
	VisionPipeline RAGPipeline = "vision_pipeline"
)

// the data we send off to rag implementations to be indexed
type SessionRAGIndexChunk struct {
	DataEntityID    string            `json:"data_entity_id"`
	Source          string            `json:"source"`
	Filename        string            `json:"filename"`
	DocumentID      string            `json:"document_id"`
	DocumentGroupID string            `json:"document_group_id"`
	ContentOffset   int               `json:"content_offset"`
	Content         string            `json:"content"`
	Metadata        map[string]string `json:"metadata"`
	Pipeline        RAGPipeline       `json:"pipeline"` // RAG providers can have different pipelines
}

// the query we post to llamaindex to get results back from a user
// prompt against a rag enabled session
type SessionRAGQuery struct {
	Prompt            string      `json:"prompt"`
	DataEntityID      string      `json:"data_entity_id"`
	DistanceThreshold float64     `json:"distance_threshold"`
	DistanceFunction  string      `json:"distance_function"`
	MaxResults        int         `json:"max_results"`
	ExhaustiveSearch  bool        `json:"exhaustive_search"`
	DocumentIDList    []string    `json:"document_id_list"` // TODO(Phil): I can see this getting out of hand, should make it more generic to handle any kind of metadata filter
	Pipeline          RAGPipeline `json:"pipeline"`         // RAG providers can have different pipelines
}

type DeleteIndexRequest struct {
	DataEntityID string `json:"data_entity_id"`
}

// the thing we load from llamaindex when we send the user prompt
// there and it does a lookup
type SessionRAGResult struct {
	ID              string            `json:"id"`
	SessionID       string            `json:"session_id"`
	InteractionID   string            `json:"interaction_id"`
	DocumentID      string            `json:"document_id"`
	DocumentGroupID string            `json:"document_group_id"`
	Filename        string            `json:"filename"`
	Source          string            `json:"source"`
	ContentOffset   int               `json:"content_offset"`
	Content         string            `json:"content"`
	Distance        float64           `json:"distance"`
	Metadata        map[string]string `json:"metadata"`
}

// gives us a quick way to add settings
type SessionMetadata struct {
	Avatar                  string              `json:"avatar"`
	Priority                bool                `json:"priority"`
	DocumentIDs             map[string]string   `json:"document_ids"`
	SessionRAGResults       []*SessionRAGResult `json:"session_rag_results"`
	DocumentGroupID         string              `json:"document_group_id"`
	ManuallyReviewQuestions bool                `json:"manually_review_questions"`
	SystemPrompt            string              `json:"system_prompt"`
	HelixVersion            string              `json:"helix_version"`
	Stream                  bool                `json:"stream"`
	// Evals are cool. Scores are strings of floats so we can distinguish ""
	// (not rated) from "0.0"
	EvalRunID               string   `json:"eval_run_id"`
	EvalUserScore           string   `json:"eval_user_score"`
	EvalUserReason          string   `json:"eval_user_reason"`
	EvalManualScore         string   `json:"eval_manual_score"`
	EvalManualReason        string   `json:"eval_manual_reason"`
	EvalAutomaticScore      string   `json:"eval_automatic_score"`
	EvalAutomaticReason     string   `json:"eval_automatic_reason"`
	EvalOriginalUserPrompts []string `json:"eval_original_user_prompts"`
	// these settings control which features of a session we want to use
	// even if we have a Lora file and RAG indexed prepared
	// we might choose to not use them (this will help our eval framework know what works the best)
	// we well as activate RAG - we also get to control some properties, e.g. which distance function to use,
	// and what the threshold for a "good" answer is
	RagEnabled          bool        `json:"rag_enabled"`           // without any user input, this will default to true
	TextFinetuneEnabled bool        `json:"text_finetune_enabled"` // without any user input, this will default to true
	RagSettings         RAGSettings `json:"rag_settings"`
	ActiveTools         []string    `json:"active_tools"`
	// when we do fine tuning or RAG, we need to know which data entity we used
	UploadedDataID string `json:"uploaded_data_entity_id"`

	// which assistant are we talking to?
	AssistantID    string            `json:"assistant_id"`
	AppQueryParams map[string]string `json:"app_query_params"` // Passing through user defined app params
}

// the packet we put a list of sessions into so pagination is supported and we know the total amount
type SessionsList struct {
	// the total number of sessions that match the query
	Counter *Counter `json:"counter"`
	// the list of sessions
	Sessions []*SessionSummary `json:"sessions"`
}

type PaginatedSessionsList struct {
	Sessions   []*SessionSummary `json:"sessions"`
	Page       int               `json:"page"`
	PageSize   int               `json:"pageSize"`
	TotalCount int64             `json:"totalCount"`
	TotalPages int               `json:"totalPages"`
}

// this is the incoming REST api struct sent from the outside world
// the user wants to do inference against a model
// we turn this into a InternalSessionRequest
type SessionChatRequest struct {
	AppID          string      `json:"app_id"`          // Assign the session settings from the specified app
	OrganizationID string      `json:"organization_id"` // The organization this session belongs to, if any
	AssistantID    string      `json:"assistant_id"`    // Which assistant are we speaking to?
	SessionID      string      `json:"session_id"`      // If empty, we will start a new session
	InteractionID  string      `json:"interaction_id"`  // If empty, we will start a new interaction
	Stream         bool        `json:"stream"`          // If true, we will stream the response
	Type           SessionType `json:"type"`            // e.g. text, image
	LoraDir        string      `json:"lora_dir"`
	SystemPrompt   string      `json:"system"`     // System message, only applicable when starting a new session
	Messages       []*Message  `json:"messages"`   // Initial messages
	Tools          []string    `json:"tools"`      // Available tools to use in the session
	Provider       Provider    `json:"provider"`   // The provider to use
	Model          string      `json:"model"`      // The model to use
	Regenerate     bool        `json:"regenerate"` // If true, we will regenerate the response for the last message
}

func (s *SessionChatRequest) Message() (string, bool) {
	if len(s.Messages) == 0 {
		return "", false
	}

	msg := s.Messages[len(s.Messages)-1]
	if msg == nil || len(msg.Content.Parts) == 0 {
		return "", false
	}

	for _, part := range msg.Content.Parts {
		if part == nil {
			continue
		}

		switch p := part.(type) {
		case string: // Case 1: Part is a simple string
			return p, true
		case TextPart: // Handle TextPart struct directly
			return p.Text, true
		// Note: ImageURLPart struct doesn't need explicit handling here if we are only looking for text.
		// It will fall through the default case of the switch, and the loop will continue.
		case map[string]interface{}: // Case 2: Part is a generic map (e.g., from JSON unmarshal)
			// Check for {"type": "text", "text": "..."}
			if typeVal, typeOk := p["type"].(string); typeOk && typeVal == "text" {
				if textVal, textOk := p["text"].(string); textOk {
					return textVal, true
				}
			}
			// If partMap["type"] is "image_url" or another non-text type,
			// this map is skipped, and the loop continues to the next part.
		}
	}

	// If the loop completes, it means no text part was found.
	// The original code returned `"", true` if parts existed but no text was found.
	// This is consistent with Test_SessionChatRequest_ImageURL expecting "", true.
	return "", true
}

func (s *SessionChatRequest) MessageContent() MessageContent {
	if len(s.Messages) == 0 {
		return MessageContent{
			ContentType: "text",
			Parts:       []any{""},
		}
	}

	return s.Messages[len(s.Messages)-1].Content
}

// the user wants to create a Lora or RAG source
// we turn this into a InternalSessionRequest
type SessionLearnRequest struct {
	Type           SessionType `json:"type"`            // e.g. text, image
	OrganizationID string      `json:"organization_id"` // The organization this session belongs to, if any
	// FINE-TUNE MODE ONLY
	DataEntityID string `json:"data_entity_id"` // The uploaded files we want to use for fine-tuning and/or RAG
	// Do we want to create a RAG data entity from this session?
	// You must provide a data entity ID for the uploaded documents if yes
	RagEnabled bool `json:"rag_enabled"`
	// Do we want to create a lora output from this session?
	// You must provide a data entity ID for the uploaded documents if yes
	TextFinetuneEnabled bool `json:"text_finetune_enabled"`
	// The settings we use for the RAG source
	RagSettings RAGSettings `json:"rag_settings"`
	// When doing RAG, allow the resulting inference session model to be specified
	DefaultRAGModel string `json:"default_rag_model"`
}

type Message struct {
	ID        string           `json:"id"` // Interaction ID
	Role      string           `json:"role"`
	Content   MessageContent   `json:"content"`
	CreatedAt time.Time        `json:"created_at,omitempty"`
	UpdatedAt time.Time        `json:"updated_at,omitempty"`
	State     InteractionState `json:"state"`
}

type MessageContentType string

const (
	MessageContentTypeText MessageContentType = "text"
)

type MessageContent struct {
	ContentType MessageContentType `json:"content_type"` // text, image_url, multimodal_text
	// Parts is a list of strings or objects. For example for text, it's a list of strings, for
	// multi-modal it can be an object:
	// "parts": [
	// 		{
	// 				"content_type": "image_asset_pointer",
	// 				"asset_pointer": "file-service://file-28uHss2LgJ8HUEEVAnXa70Tg",
	// 				"size_bytes": 185427,
	// 				"width": 2048,
	// 				"height": 1020,
	// 				"fovea": null,
	// 				"metadata": null
	// 		},
	// 		"what is in the image?"
	// ]
	Parts []any `json:"parts"`
}

// this is the internal struct used to manage session creation
type InternalSessionRequest struct {
	ID                      string
	Stream                  bool
	Type                    SessionType
	SystemPrompt            string
	ParentSession           string
	ParentApp               string // tools will get pulled in from here in the controller
	OrganizationID          string // the organization this session belongs to, if any
	AssistantID             string // target a specific assistant - defaults to "0" (i.e. the first assistant)
	ModelName               string
	Owner                   string
	OwnerType               OwnerType
	UserInteractions        []*Interaction
	Priority                bool
	ManuallyReviewQuestions bool
	RAGEnabled              bool
	TextFinetuneEnabled     bool
	RAGSettings             RAGSettings
	ActiveTools             []string
	ResponseFormat          ResponseFormat
	UploadedDataID          string
	RAGSourceID             string
	LoraID                  string
	AppQueryParams          map[string]string // Passing through user defined app params
	// Model function calling, not to be mistaken with Helix tools
	Tools []openai.Tool `json:"tools"`

	// This can be either a string or an ToolChoice object.
	ToolChoice any `json:"tool_choice,omitempty"`
}

type UpdateSessionRequest struct {
	SessionID       string
	UserInteraction *Interaction
	SessionMode     SessionMode
}

type Session struct {
	ID string `json:"id"`
	// name that goes in the UI - ideally autogenerated by AI but for now can be
	// named manually
	Name          string    `json:"name"`
	Created       time.Time `json:"created"`
	Updated       time.Time `json:"updated"`
	ParentSession string    `json:"parent_session"`
	// the app this session was spawned from
	// TODO: rename to AppID
	ParentApp string `json:"parent_app"`
	// the organization this session belongs to, if any
	OrganizationID string          `json:"organization_id" gorm:"index"`
	Metadata       SessionMetadata `json:"config" gorm:"column:config;type:jsonb"` // named config for backward compat
	// e.g. inference, finetune
	Mode SessionMode `json:"mode"`
	// e.g. text, image
	Type SessionType `json:"type"`
	// huggingface model name e.g. mistralai/Mistral-7B-Instruct-v0.1 or
	// stabilityai/stable-diffusion-xl-base-1.0
	Provider  string `json:"provider"`
	ModelName string `json:"model_name"`
	// if type == finetune, we record a filestore path to e.g. lora file here
	// currently the only place you can do inference on a finetune is within the
	// session where the finetune was generated
	LoraDir string `json:"lora_dir"`
	// for now we just whack the entire history of the interaction in here, json
	// style
	Interactions []*Interaction `json:"interactions" gorm:"constraint:OnUpdate:CASCADE,OnDelete:CASCADE"`
	GenerationID int            `json:"generation_id"` // Current generation ID
	// uuid of owner entity
	Owner string `json:"owner"`
	// e.g. user, system, org
	OwnerType OwnerType `json:"owner_type"`

	Trigger string `json:"trigger"`
}

func (m SessionMetadata) Value() (driver.Value, error) {
	j, err := json.Marshal(m)
	return j, err
}

func (m *SessionMetadata) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New("type assertion .([]byte) failed")
	}
	var result SessionMetadata
	if err := json.Unmarshal(source, &result); err != nil {
		return err
	}
	*m = result
	return nil
}

func (SessionMetadata) GormDataType() string {
	return "json"
}

// things we can change about a session that are not interaction related
type SessionMetaUpdate struct {
	ID   string `json:"id"`
	Name string `json:"name"`
	// uuid of owner entity
	Owner string `json:"owner"`
	// e.g. user, system, org
	OwnerType OwnerType `json:"owner_type"`
}

type SessionFilterModel struct {
	Mode      SessionMode `json:"mode"`
	ModelName string      `json:"model_name"`
	LoraDir   string      `json:"lora_dir"`
}

type Duration time.Duration

func (d Duration) MarshalJSON() ([]byte, error) {
	return json.Marshal(time.Duration(d).String())
}

func (d *Duration) UnmarshalJSON(b []byte) error {
	var v interface{}
	if err := json.Unmarshal(b, &v); err != nil {
		return err
	}
	switch value := v.(type) {
	case string:
		tmp, err := time.ParseDuration(value)
		if err != nil {
			return err
		}
		*d = Duration(tmp)
		return nil
	default:
		return errors.New("invalid duration")
	}
}

type SessionFilter struct {
	// e.g. inference, finetune
	Mode SessionMode `json:"mode"`
	// e.g. text, image
	Type SessionType `json:"type"`
	// huggingface model name e.g. mistralai/Mistral-7B-Instruct-v0.1 or
	// stabilityai/stable-diffusion-xl-base-1.0
	ModelName string `json:"model_name"`
	// the filestore path to the file being used for finetuning
	LoraDir string `json:"lora_dir"`
	// this means "only give me sessions that will fit in this much ram"
	Memory uint64 `json:"memory"`

	Runtime InferenceRuntime `json:"runtime"`

	// the list of model name / mode combos that we should skip over
	// normally used by runners that are running multiple types in parallel
	// who don't want another version of what they are already running
	Reject []SessionFilterModel `json:"reject"`

	// only accept sessions that were created more than this duration ago
	Older Duration `json:"older"`
}

type InferenceRequestFilter struct {
	ModelName string        `json:"model_name"`
	Memory    uint64        `json:"memory"`
	Older     time.Duration `json:"older"`
}

type ApiKey struct { //nolint:revive
	Created   time.Time       `json:"created"`
	Owner     string          `json:"owner"`
	OwnerType OwnerType       `json:"owner_type"`
	Key       string          `json:"key" gorm:"primaryKey"`
	Name      string          `json:"name"`
	Type      APIKeyType      `json:"type" gorm:"default:api"`
	AppID     *sql.NullString `json:"app_id"`
}

type OwnerContext struct {
	Owner     string
	OwnerType OwnerType
}

type StripeUser struct {
	StripeCustomerID string
	UserID           string
	Email            string
	SubscriptionID   string
	SubscriptionURL  string
}

// this is given to the frontend as user context
type UserStatus struct {
	Admin  bool       `json:"admin"`
	User   string     `json:"user"`
	Config UserConfig `json:"config"`
}

// a single envelope that is broadcast to users
type WebsocketEvent struct {
	Type               WebsocketEventType          `json:"type"`
	SessionID          string                      `json:"session_id"`
	InteractionID      string                      `json:"interaction_id"`
	Owner              string                      `json:"owner"`
	Session            *Session                    `json:"session"`
	WorkerTaskResponse *RunnerTaskResponse         `json:"worker_task_response"`
	InferenceResponse  *RunnerLLMInferenceResponse `json:"inference_response"`
	StepInfo           *StepInfo                   `json:"step_info"`
}

type StepInfoType string

const (
	StepInfoTypeWebSearch = "web_search"
	StepInfoTypeRAG       = "rag"
	StepInfoTypeToolUse   = "tool_use"
	StepInfoTypeThinking  = "thinking"
)

type StepInfo struct {
	ID            string          `json:"id" gorm:"primaryKey"`
	Created       time.Time       `json:"created"`
	Updated       time.Time       `json:"updated"`
	AppID         string          `json:"app_id" gorm:"index:idx_app_interaction,priority:1"`
	SessionID     string          `json:"session_id" gorm:"index"`
	InteractionID string          `json:"interaction_id" gorm:"index:idx_app_interaction,priority:2"`
	Name          string          `json:"name"`
	Icon          string          `json:"icon"` // Either Material UI icon, emoji or SVG. Leave empty for default
	Type          StepInfoType    `json:"type"`
	Message       string          `json:"message"`
	Error         string          `json:"error"`
	Details       StepInfoDetails `json:"details" gorm:"type:jsonb"` // That were used to call the tool
	DurationMs    int64           `json:"duration_ms"`               // How long the step took in milliseconds (useful for API calls, database queries, etc.)
}

type StepInfoDetails struct {
	Arguments map[string]interface{} `json:"arguments"`
	// TODO: OAuth tokens supplied or not
}

func (t StepInfoDetails) Value() (driver.Value, error) {
	j, err := json.Marshal(t)
	return j, err
}

func (t *StepInfoDetails) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New("type assertion .([]byte) failed")
	}
	var result StepInfoDetails
	if err := json.Unmarshal(source, &result); err != nil {
		return err
	}
	*t = result
	return nil
}

func (StepInfoDetails) GormDataType() string {
	return "json"
}

// the context of a long running python process
// on a runner - this will be used to inject the env
// into the cmd returned by the model instance.GetCommand() function
type RunnerProcessConfig struct {
	// the id of the model instance
	InstanceID string `json:"instance_id"`
	// the URL to ask for more tasks
	// this will pop the task from the queue
	NextTaskURL string `json:"next_task_url"`
	// the URL to ask for what the session is (e.g. to know what finetune_file to load)
	// this is readonly and will not pop the session(task) from the queue
	InitialSessionURL string `json:"initial_session_url"`
	MockRunner        bool
	MockRunnerError   string
	MockRunnerDelay   int
	Port              int
}

// a session will run "tasks" on runners
// task's job is to take the most recent user interaction
// and add a response to it in the form of a assistant interaction
// the api controller will have already appended the assistant interaction
// to the very end of the Session.Interactions list
// our job is to fill in the Message and/or Files field of that interaction
type RunnerTask struct {
	SessionID string `json:"session_id"`
	// the string that we are calling the prompt that we will feed into the model
	Prompt string `json:"prompt"`

	// the directory that contains the lora training files
	LoraDir string `json:"lora_dir"`

	// this is the directory that contains the files used for fine tuning
	// i.e. it's the user files that will be the input to a finetune session
	DatasetDir string `json:"dataset_dir"`
}

type RunnerTaskResponse struct {
	// the python code must submit these fields back to the runner api
	Type      WorkerTaskResponseType `json:"type"`
	SessionID string                 `json:"session_id"`
	// this should be the latest assistant interaction
	// it is filled in by the model instance
	// based on currentSession
	InteractionID string `json:"interaction_id"`
	Owner         string `json:"owner"`
	// which fields the python code decides to fill in here depends
	// on what the type of model it is
	Message  string   `json:"message,omitempty"`  // e.g. Prove pythagoras
	Progress int      `json:"progress,omitempty"` // e.g. 0-100
	Status   string   `json:"status,omitempty"`   // e.g. updating X
	Files    []string `json:"files,omitempty"`    // list of filepath paths
	LoraDir  string   `json:"lora_dir,omitempty"`
	Error    string   `json:"error,omitempty"`
	Usage    Usage    `json:"usage,omitempty"`
	Done     bool     `json:"done,omitempty"`
	// For Role=assistant prompts this may be set to the tool calls generated by the model, such as function calls.
	ToolCalls []openai.ToolCall `json:"tool_calls,omitempty"`

	// For Role=tool prompts this should be set to the ID given in the assistant's prior request to call a tool.
	ToolCallID string `json:"tool_call_id,omitempty"`
}

type Usage struct {
	PromptTokens     int   `json:"prompt_tokens"`
	CompletionTokens int   `json:"completion_tokens"`
	TotalTokens      int   `json:"total_tokens"`
	DurationMs       int64 `json:"duration_ms"` // How long the request took in milliseconds
}

// this is returned by the api server so that clients can see what
// config it's using e.g. filestore prefix
type ServerConfigForFrontend struct {
	// used to prepend onto raw filestore paths to download files
	// the filestore path will have the user info in it - i.e.
	// it's a low level filestore path
	// if we are using an object storage thing - then this URL
	// can be the prefix to the bucket
	FilestorePrefix                        string               `json:"filestore_prefix"`
	StripeEnabled                          bool                 `json:"stripe_enabled"`  // Stripe top-ups enabled
	BillingEnabled                         bool                 `json:"billing_enabled"` // Charging for usage
	SentryDSNFrontend                      string               `json:"sentry_dsn_frontend"`
	GoogleAnalyticsFrontend                string               `json:"google_analytics_frontend"`
	EvalUserID                             string               `json:"eval_user_id"`
	ToolsEnabled                           bool                 `json:"tools_enabled"`
	AppsEnabled                            bool                 `json:"apps_enabled"`
	RudderStackWriteKey                    string               `json:"rudderstack_write_key"`
	RudderStackDataPlaneURL                string               `json:"rudderstack_data_plane_url"`
	DisableLLMCallLogging                  bool                 `json:"disable_llm_call_logging"`
	Version                                string               `json:"version"`
	LatestVersion                          string               `json:"latest_version"`
	DeploymentID                           string               `json:"deployment_id"`
	License                                *FrontendLicenseInfo `json:"license,omitempty"`
	OrganizationsCreateEnabledForNonAdmins bool                 `json:"organizations_create_enabled_for_non_admins"`
}

// a short version of a session that we keep for the dashboard
type SessionSummary struct {
	// these are all values of the last interaction
	Created   time.Time `json:"created"`
	Updated   time.Time `json:"updated"`
	SessionID string    `json:"session_id"`
	Name      string    `json:"name"`
	// InteractionID string      `json:"interaction_id"`
	ModelName string      `json:"model_name"`
	Type      SessionType `json:"type"`
	Owner     string      `json:"owner"`

	// this is either the prompt or the summary of the training data
	Summary        string `json:"summary"`
	Priority       bool   `json:"priority"`
	AppID          string `json:"app_id,omitempty"`
	OrganizationID string `json:"organization_id,omitempty"`
}

type WorkloadSummary struct {
	ID        string    `json:"id"`
	CreatedAt time.Time `json:"created"`
	UpdatedAt time.Time `json:"updated"`
	ModelName string    `json:"model_name"`
	Mode      string    `json:"mode"`
	Runtime   string    `json:"runtime"`
	LoraDir   string    `json:"lora_dir"`
	Summary   string    `json:"summary"`
}

type DashboardData struct {
	Runners                   []*DashboardRunner          `json:"runners"`
	Queue                     []*WorkloadSummary          `json:"queue"`
	SchedulingDecisions       []*SchedulingDecision       `json:"scheduling_decisions"`
	GlobalAllocationDecisions []*GlobalAllocationDecision `json:"global_allocation_decisions"`
}

// GPUMemoryDataPoint represents a single point in time for GPU memory tracking
type GPUMemoryDataPoint struct {
	Timestamp     time.Time `json:"timestamp"`
	GPUIndex      int       `json:"gpu_index"`
	AllocatedMB   uint64    `json:"allocated_mb"`    // Memory allocated by Helix scheduler
	ActualUsedMB  uint64    `json:"actual_used_mb"`  // Actual memory used (from nvidia-smi)
	ActualFreeMB  uint64    `json:"actual_free_mb"`  // Actual free memory (from nvidia-smi)
	ActualTotalMB uint64    `json:"actual_total_mb"` // Total GPU memory
}

// SchedulingEvent represents a scheduling event for correlation with memory usage
type SchedulingEvent struct {
	Timestamp   time.Time `json:"timestamp"`
	EventType   string    `json:"event_type"` // "slot_created", "slot_deleted", "eviction", "stabilization_start", "stabilization_end"
	SlotID      string    `json:"slot_id,omitempty"`
	ModelName   string    `json:"model_name,omitempty"`
	Runtime     string    `json:"runtime,omitempty"`
	GPUIndices  []int     `json:"gpu_indices,omitempty"`
	MemoryMB    uint64    `json:"memory_mb,omitempty"`
	Description string    `json:"description,omitempty"`
}

// GPUMemoryReading represents a single memory reading during stabilization
type GPUMemoryReading struct {
	PollNumber  int    `json:"poll_number"`
	MemoryMB    uint64 `json:"memory_mb"`
	DeltaMB     int64  `json:"delta_mb"`
	StableCount int    `json:"stable_count"`
	IsStable    bool   `json:"is_stable"`
}

// GPUMemoryStabilizationEvent represents a single GPU memory stabilization event
type GPUMemoryStabilizationEvent struct {
	Timestamp              time.Time          `json:"timestamp"`
	Context                string             `json:"context"` // "startup" or "deletion"
	SlotID                 string             `json:"slot_id,omitempty"`
	Runtime                string             `json:"runtime,omitempty"`
	TimeoutSeconds         int                `json:"timeout_seconds"`
	PollIntervalMs         int                `json:"poll_interval_ms"`
	RequiredStablePolls    int                `json:"required_stable_polls"`
	MemoryDeltaThresholdMB uint64             `json:"memory_delta_threshold_mb"`
	Success                bool               `json:"success"`
	PollsTaken             int                `json:"polls_taken"`
	TotalWaitSeconds       int                `json:"total_wait_seconds"`
	StabilizedMemoryMB     uint64             `json:"stabilized_memory_mb,omitempty"`
	ErrorMessage           string             `json:"error_message,omitempty"`
	MemoryReadings         []GPUMemoryReading `json:"memory_readings,omitempty"`
}

// GPUMemoryStats tracks GPU memory stabilization statistics
type GPUMemoryStats struct {
	TotalStabilizations      int                           `json:"total_stabilizations"`
	SuccessfulStabilizations int                           `json:"successful_stabilizations"`
	FailedStabilizations     int                           `json:"failed_stabilizations"`
	LastStabilization        *time.Time                    `json:"last_stabilization,omitempty"`
	RecentEvents             []GPUMemoryStabilizationEvent `json:"recent_events"` // Last 20 events
	AverageWaitTimeSeconds   float64                       `json:"average_wait_time_seconds"`
	MaxWaitTimeSeconds       int                           `json:"max_wait_time_seconds"`
	MinWaitTimeSeconds       int                           `json:"min_wait_time_seconds"`
	MemoryTimeSeries         []GPUMemoryDataPoint          `json:"memory_time_series"` // Last 10 minutes of memory data
	SchedulingEvents         []SchedulingEvent             `json:"scheduling_events"`  // Last 10 minutes of scheduling events
}

type DashboardRunner struct {
	ID              string               `json:"id"`
	Created         time.Time            `json:"created"`
	Updated         time.Time            `json:"updated"`
	Version         string               `json:"version"`
	TotalMemory     uint64               `json:"total_memory"`
	FreeMemory      uint64               `json:"free_memory"`
	UsedMemory      uint64               `json:"used_memory"`
	AllocatedMemory uint64               `json:"allocated_memory"`
	GPUCount        int                  `json:"gpu_count"` // Number of GPUs detected
	GPUs            []*GPUStatus         `json:"gpus"`      // Per-GPU memory status
	Labels          map[string]string    `json:"labels"`
	Slots           []*RunnerSlot        `json:"slots"`
	MemoryString    string               `json:"memory_string"`
	Models          []*RunnerModelStatus `json:"models"`
	ProcessStats    interface{}          `json:"process_stats,omitempty"`    // Process tracking and cleanup statistics
	GPUMemoryStats  *GPUMemoryStats      `json:"gpu_memory_stats,omitempty"` // GPU memory stabilization statistics
}

type GlobalSchedulingDecision struct {
	Created       time.Time     `json:"created"`
	RunnerID      string        `json:"runner_id"`
	SessionID     string        `json:"session_id"`
	InteractionID string        `json:"interaction_id"`
	ModelName     string        `json:"model_name"`
	Mode          SessionMode   `json:"mode"`
	Filter        SessionFilter `json:"filter"`
}

// AllocationPlanView represents a plan option for visualization
type AllocationPlanView struct {
	ID                  string         `json:"id"`
	RunnerID            string         `json:"runner_id"`
	GPUs                []int          `json:"gpus"`
	GPUCount            int            `json:"gpu_count"`
	IsMultiGPU          bool           `json:"is_multi_gpu"`
	TotalMemoryRequired uint64         `json:"total_memory_required"`
	MemoryPerGPU        uint64         `json:"memory_per_gpu"`
	Cost                int            `json:"cost"`
	RequiresEviction    bool           `json:"requires_eviction"`
	EvictionsNeeded     []string       `json:"evictions_needed"` // Slot IDs
	TensorParallelSize  int            `json:"tensor_parallel_size"`
	Runtime             Runtime        `json:"runtime"`
	IsValid             bool           `json:"is_valid"`
	ValidationError     string         `json:"validation_error,omitempty"`
	RunnerMemoryState   map[int]uint64 `json:"runner_memory_state"` // GPU index -> allocated memory
	RunnerCapacity      map[int]uint64 `json:"runner_capacity"`     // GPU index -> total memory
}

// GlobalAllocationDecision represents a complete global allocation decision for visualization
type GlobalAllocationDecision struct {
	ID         string    `json:"id"`
	Created    time.Time `json:"created"`
	WorkloadID string    `json:"workload_id"`
	SessionID  string    `json:"session_id"`
	ModelName  string    `json:"model_name"`
	Runtime    Runtime   `json:"runtime"`

	// All plans considered
	ConsideredPlans []*AllocationPlanView `json:"considered_plans"`
	SelectedPlan    *AllocationPlanView   `json:"selected_plan"`

	// Timing information
	PlanningTimeMs  int64 `json:"planning_time_ms"`
	ExecutionTimeMs int64 `json:"execution_time_ms"`
	TotalTimeMs     int64 `json:"total_time_ms"`

	// Decision outcome
	Success      bool   `json:"success"`
	Reason       string `json:"reason"`
	ErrorMessage string `json:"error_message,omitempty"`

	// Global state snapshots
	BeforeState map[string]*RunnerStateView `json:"before_state"`
	AfterState  map[string]*RunnerStateView `json:"after_state"`

	// Decision metadata
	TotalRunnersEvaluated int     `json:"total_runners_evaluated"`
	TotalPlansGenerated   int     `json:"total_plans_generated"`
	OptimizationScore     float64 `json:"optimization_score"` // How optimal the final decision was
}

// RunnerStateView represents a runner's state for visualization
type RunnerStateView struct {
	RunnerID    string            `json:"runner_id"`
	GPUStates   map[int]*GPUState `json:"gpu_states"` // GPU index -> state
	TotalSlots  int               `json:"total_slots"`
	ActiveSlots int               `json:"active_slots"`
	WarmSlots   int               `json:"warm_slots"`
	IsConnected bool              `json:"is_connected"`
}

// GPUState represents a single GPU's state
type GPUState struct {
	Index           int      `json:"index"`
	TotalMemory     uint64   `json:"total_memory"`
	AllocatedMemory uint64   `json:"allocated_memory"`
	FreeMemory      uint64   `json:"free_memory"`
	ActiveSlots     []string `json:"active_slots"` // Slot IDs using this GPU
	Utilization     float64  `json:"utilization"`  // 0.0 - 1.0
}

// keep track of the state of the data prep
// no error means "success"
// we have a map[string][]DataPrepChunk
// where string is filename
type DataPrepChunk struct {
	Index         int    `json:"index"`
	PromptName    string `json:"prompt_name"`
	QuestionCount int    `json:"question_count"`
	Error         string `json:"error"`
}

// the thing we get from the LLM's
type DataPrepTextQuestionRaw struct {
	Question string `json:"question" yaml:"question"`
	Answer   string `json:"answer" yaml:"answer"`
}

type DataPrepTextQuestionPart struct {
	From  string `json:"from"`
	Value string `json:"value"`
}

type DataPrepTextQuestion struct {
	Conversations []DataPrepTextQuestionPart `json:"conversations"`
}

type Counter struct {
	Count int64 `json:"count"`
}

type ToolHistoryMessage struct {
	Role    string
	Content string
}

func HistoryFromChatCompletionRequest(req openai.ChatCompletionRequest) []*ToolHistoryMessage {
	var history []*ToolHistoryMessage

	// Copy the messages from the request into history messages
	for _, message := range req.Messages {
		contentString, err := GetMessageText(&message)
		if err != nil {
			continue
		}

		if message.Role == openai.ChatMessageRoleSystem {
			// it is a VERY bad idea to include >1 system message when talking to an LLM
			// https://x.com/lmarsden/status/1826406206996693431
			continue
		}
		history = append(history, &ToolHistoryMessage{
			Role:    message.Role,
			Content: contentString,
		})
	}

	return history
}

// GetMessageText extracts the plain text content from an OpenAI chat message
// of any type (user, assistant, or developer message)
func GetMessageText(message *openai.ChatCompletionMessage) (string, error) {
	// For multi-content messages
	if len(message.MultiContent) > 0 {
		var builder strings.Builder
		for _, part := range message.MultiContent {
			if part.Type == "text" {
				builder.WriteString(part.Text)
			}
		}
		return builder.String(), nil
	}

	if message.Content == "" {
		return "", fmt.Errorf("message %+v content is empty", message)
	}

	// For simple string content
	if message.Content != "" {
		return message.Content, nil
	}

	return "", fmt.Errorf("unsupported message type %+v", message)
}

type PaginatedLLMCalls struct {
	Calls      []*LLMCall `json:"calls"`
	Page       int        `json:"page"`
	PageSize   int        `json:"pageSize"`
	TotalCount int64      `json:"totalCount"`
	TotalPages int        `json:"totalPages"`
}

type PaginatedInteractions struct {
	Interactions []*Interaction `json:"interactions"`
	Page         int            `json:"page"`
	PageSize     int            `json:"pageSize"`
	TotalCount   int64          `json:"totalCount"`
	TotalPages   int            `json:"totalPages"`
}

type PaginatedUsersList struct {
	Users      []*User `json:"users"`
	Page       int     `json:"page"`
	PageSize   int     `json:"pageSize"`
	TotalCount int64   `json:"totalCount"`
	TotalPages int     `json:"totalPages"`
}

type ToolType string

const (
	ToolTypeAPI         ToolType = "api"
	ToolTypeBrowser     ToolType = "browser"
	ToolTypeGPTScript   ToolType = "gptscript"
	ToolTypeZapier      ToolType = "zapier"
	ToolTypeCalculator  ToolType = "calculator"
	ToolTypeEmail       ToolType = "email"
	ToolTypeWebSearch   ToolType = "web_search"
	ToolTypeAzureDevOps ToolType = "azure_devops"
	ToolTypeMCP         ToolType = "mcp"
)

type Tool struct {
	ID           string     `json:"id" gorm:"primaryKey"`
	Name         string     `json:"name"`
	Description  string     `json:"description"`
	SystemPrompt string     `json:"system_prompt"` // E.g. As a restaurant expert, you provide personalized restaurant recommendations
	ToolType     ToolType   `json:"tool_type"`
	Global       bool       `json:"global"`
	Config       ToolConfig `json:"config" gorm:"jsonb"`
}

type ToolConfig struct {
	API         *ToolAPIConfig         `json:"api"`
	GPTScript   *ToolGPTScriptConfig   `json:"gptscript"`
	Zapier      *ToolZapierConfig      `json:"zapier"`
	Browser     *ToolBrowserConfig     `json:"browser"`
	WebSearch   *ToolWebSearchConfig   `json:"web_search"`
	Calculator  *ToolCalculatorConfig  `json:"calculator"`
	Email       *ToolEmailConfig       `json:"email"`
	AzureDevOps *ToolAzureDevOpsConfig `json:"azure_devops"`
	MCP         *ToolMCPClientConfig   `json:"mcp"`
}

type ToolMCPClientConfig struct {
	Name          string            `json:"name" yaml:"name"`
	Description   string            `json:"description" yaml:"description"`
	Enabled       bool              `json:"enabled" yaml:"enabled"`
	URL           string            `json:"url" yaml:"url"`
	Headers       map[string]string `json:"headers,omitempty" yaml:"headers,omitempty"`
	OAuthProvider string            `json:"oauth_provider,omitempty" yaml:"oauth_provider,omitempty"`
	OAuthScopes   []string          `json:"oauth_scopes,omitempty" yaml:"oauth_scopes,omitempty"` // Required OAuth scopes for this API

	Tools []mcp.Tool `json:"tools" yaml:"tools"`
}

type ToolAzureDevOpsConfig struct {
	Enabled             bool   `json:"enabled" yaml:"enabled"`
	OrganizationURL     string `json:"organization_url" yaml:"organization_url"`
	PersonalAccessToken string `json:"personal_access_token" yaml:"personal_access_token"`
}

type ToolBrowserConfig struct {
	Enabled                bool `json:"enabled" yaml:"enabled"`
	MarkdownPostProcessing bool `json:"markdown_post_processing" yaml:"markdown_post_processing"` // If true, the browser will return the HTML as markdown
	ProcessOutput          bool `json:"process_output" yaml:"process_output"`                     // If true, the browser will process the output of the tool call before returning it to the top loop. Useful for skills that return structured data such as Browser,
	// TODO: whitelist URLs?
}

type ToolWebSearchConfig struct {
	Enabled    bool `json:"enabled" yaml:"enabled"`
	MaxResults int  `json:"max_results" yaml:"max_results"`
}

type ToolEmailConfig struct {
	Enabled         bool   `json:"enabled" yaml:"enabled"`
	TemplateExample string `json:"template_example" yaml:"template_example"`
}

type ToolCalculatorConfig struct {
	Enabled bool `json:"enabled" yaml:"enabled"`
}

func (t ToolConfig) Value() (driver.Value, error) {
	j, err := json.Marshal(t)
	return j, err
}

func (t *ToolConfig) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New("type assertion .([]byte) failed")
	}
	var result ToolConfig
	if err := json.Unmarshal(source, &result); err != nil {
		return err
	}
	*t = result
	return nil
}

func (ToolConfig) GormDataType() string {
	return "json"
}

type ToolAPIConfig struct {
	URL     string           `json:"url" yaml:"url"` // Server override
	Schema  string           `json:"schema" yaml:"schema"`
	Actions []*ToolAPIAction `json:"actions" yaml:"actions"` // Read-only, parsed from schema on creation

	Headers    map[string]string `json:"headers" yaml:"headers"`         // Headers (authentication, etc)
	Query      map[string]string `json:"query" yaml:"query"`             // Query parameters that will be always set
	PathParams map[string]string `json:"path_params" yaml:"path_params"` // Path parameters that will be substituted in URLs

	SystemPrompt string `json:"system_prompt" yaml:"system_prompt"` // System prompt to guide the AI when using this API

	RequestPrepTemplate     string `json:"request_prep_template" yaml:"request_prep_template"`         // Template for request preparation, leave empty for default
	ResponseSuccessTemplate string `json:"response_success_template" yaml:"response_success_template"` // Template for successful response, leave empty for default
	ResponseErrorTemplate   string `json:"response_error_template" yaml:"response_error_template"`     // Template for error response, leave empty for default

	// if true, unknown keys in the response body will be removed before
	// returning to the agent for interpretation
	SkipUnknownKeys bool `json:"skip_unknown_keys" yaml:"skip_unknown_keys"`

	// Transform JSON into readable text to reduce the
	// size of the response body
	TransformOutput bool `json:"transform_output" yaml:"transform_output"`

	// OAuth configuration
	OAuthProvider string   `json:"oauth_provider" yaml:"oauth_provider"` // The name of the OAuth provider to use for authentication
	OAuthScopes   []string `json:"oauth_scopes" yaml:"oauth_scopes"`     // Required OAuth scopes for this API

	Model string `json:"model" yaml:"model"`
}

// ToolApiConfig is parsed from the OpenAPI spec
type ToolAPIAction struct {
	Name        string `json:"name" yaml:"name"`
	Description string `json:"description" yaml:"description"`
	Method      string `json:"method" yaml:"method"`
	Path        string `json:"path" yaml:"path"`
}

type ToolGPTScriptConfig struct {
	Script    string `json:"script"`     // Program code
	ScriptURL string `json:"script_url"` // URL to download the script
}

type ToolZapierConfig struct {
	APIKey        string `json:"api_key"`
	Model         string `json:"model"`
	MaxIterations int    `json:"max_iterations"`
}

type AssistantGPTScript struct {
	Name        string `json:"name" yaml:"name"`
	Description string `json:"description" yaml:"description"` // When to use this tool (required)
	File        string `json:"file" yaml:"file"`
	Content     string `json:"content" yaml:"content"`
}

type AssistantZapier struct {
	Name          string `json:"name" yaml:"name"`
	Description   string `json:"description" yaml:"description"`
	APIKey        string `json:"api_key" yaml:"api_key"`
	Model         string `json:"model" yaml:"model"`
	MaxIterations int    `json:"max_iterations" yaml:"max_iterations"`
}

type AssistantMCP struct {
	Name          string            `json:"name" yaml:"name"`
	Description   string            `json:"description" yaml:"description"`
	URL           string            `json:"url" yaml:"url"`
	Headers       map[string]string `json:"headers,omitempty" yaml:"headers,omitempty"`
	OAuthProvider string            `json:"oauth_provider,omitempty" yaml:"oauth_provider,omitempty"` // The name of the OAuth provider to use for authentication
	OAuthScopes   []string          `json:"oauth_scopes,omitempty" yaml:"oauth_scopes,omitempty"`     // Required OAuth scopes for this API
	Tools         []mcp.Tool        `json:"tools" yaml:"tools"`
}

type AssistantAPI struct {
	Name        string            `json:"name" yaml:"name"`
	Description string            `json:"description" yaml:"description"`
	Schema      string            `json:"schema" yaml:"schema"`
	URL         string            `json:"url" yaml:"url"`
	Headers     map[string]string `json:"headers,omitempty" yaml:"headers,omitempty"`
	Query       map[string]string `json:"query,omitempty" yaml:"query,omitempty"`
	PathParams  map[string]string `json:"path_params,omitempty" yaml:"path_params,omitempty"`
	// if true, unknown keys in the response body will be removed before
	// returning to the agent for interpretation
	SkipUnknownKeys bool `json:"skip_unknown_keys" yaml:"skip_unknown_keys"`

	// Transform JSON into readable text to reduce the
	// size of the response body
	TransformOutput bool `json:"transform_output" yaml:"transform_output"`

	SystemPrompt string `json:"system_prompt,omitempty" yaml:"system_prompt,omitempty"`

	RequestPrepTemplate     string `json:"request_prep_template,omitempty" yaml:"request_prep_template,omitempty"`
	ResponseSuccessTemplate string `json:"response_success_template,omitempty" yaml:"response_success_template,omitempty"`
	ResponseErrorTemplate   string `json:"response_error_template,omitempty" yaml:"response_error_template,omitempty"`

	// OAuth configuration
	OAuthProvider string   `json:"oauth_provider,omitempty" yaml:"oauth_provider,omitempty"` // The name of the OAuth provider to use for authentication
	OAuthScopes   []string `json:"oauth_scopes,omitempty" yaml:"oauth_scopes,omitempty"`     // Required OAuth scopes for this API
}

type AssistantAzureDevOps struct {
	Enabled             bool   `json:"enabled" yaml:"enabled"`
	OrganizationURL     string `json:"organization_url" yaml:"organization_url"`
	PersonalAccessToken string `json:"personal_access_token" yaml:"personal_access_token"`
}

// apps are a collection of assistants
// the APIs and GPTScripts are both processed into a single list of Tools
type AssistantConfig struct {
	ID          string `json:"id,omitempty" yaml:"id,omitempty"`
	Name        string `json:"name,omitempty" yaml:"name,omitempty"`
	Description string `json:"description,omitempty" yaml:"description,omitempty"`
	Avatar      string `json:"avatar,omitempty" yaml:"avatar,omitempty"`
	Image       string `json:"image,omitempty" yaml:"image,omitempty"`
	Provider    string `json:"provider,omitempty" yaml:"provider,omitempty"`
	Model       string `json:"model,omitempty" yaml:"model,omitempty"`

	// ConversationStarters is a list of messages that will be presented to the user
	// when a new session is about to be launched. Use this to showcase the capabilities of the assistant.
	ConversationStarters []string `json:"conversation_starters,omitempty" yaml:"conversation_starters,omitempty"`

	// AgentMode triggers the use of the agent loop
	AgentMode     bool `json:"agent_mode" yaml:"agent_mode"`
	MaxIterations int  `json:"max_iterations" yaml:"max_iterations"`

	ReasoningModelProvider string `json:"reasoning_model_provider" yaml:"reasoning_model_provider"`
	ReasoningModel         string `json:"reasoning_model" yaml:"reasoning_model"`
	ReasoningModelEffort   string `json:"reasoning_model_effort" yaml:"reasoning_model_effort"`

	GenerationModelProvider string `json:"generation_model_provider" yaml:"generation_model_provider"`
	GenerationModel         string `json:"generation_model" yaml:"generation_model"`

	SmallReasoningModelProvider string `json:"small_reasoning_model_provider" yaml:"small_reasoning_model_provider"`
	SmallReasoningModel         string `json:"small_reasoning_model" yaml:"small_reasoning_model"`
	SmallReasoningModelEffort   string `json:"small_reasoning_model_effort" yaml:"small_reasoning_model_effort"`

	SmallGenerationModelProvider string `json:"small_generation_model_provider" yaml:"small_generation_model_provider"`
	SmallGenerationModel         string `json:"small_generation_model" yaml:"small_generation_model"`

	SystemPrompt string `json:"system_prompt,omitempty" yaml:"system_prompt,omitempty"`

	RAGSourceID string `json:"rag_source_id,omitempty" yaml:"rag_source_id,omitempty"`
	LoraID      string `json:"lora_id,omitempty" yaml:"lora_id,omitempty"`

	Memory bool `json:"memory,omitempty" yaml:"memory,omitempty"` // Enable/disable user based memory for the agent

	// ContextLimit - the number of messages to include in the context for the AI assistant.
	// When set to 1, the AI assistant will only see and remember the most recent message.
	ContextLimit int `json:"context_limit,omitempty" yaml:"context_limit,omitempty"`

	// Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
	// 0.01 - precise
	// 1 - neutral
	// 2 - creative
	Temperature float32 `json:"temperature,omitempty" yaml:"temperature,omitempty"`

	// How much to penalize new tokens based on whether they appear in the text so far.
	// Increases the model's likelihood to talk about new topics
	// 0 - balanced
	// 2 - open minded
	PresencePenalty float32 `json:"presence_penalty,omitempty" yaml:"presence_penalty,omitempty"`

	// How much to penalize new tokens based on their frequency in the text so far.
	// Increases the model's likelihood to talk about new topics
	// 0 - balanced
	// 2 - less repetitive
	FrequencyPenalty float32 `json:"frequency_penalty,omitempty" yaml:"frequency_penalty,omitempty"`

	// An alternative to sampling with temperature, called nucleus sampling,
	// where the model considers the results of the tokens with top_p probability mass.
	// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	// 0 - balanced
	// 2 - more creative
	TopP float32 `json:"top_p,omitempty" yaml:"top_p,omitempty"`

	// The maximum number of tokens to generate before stopping.
	MaxTokens int `json:"max_tokens,omitempty" yaml:"max_tokens,omitempty"`

	// Controls effort on reasoning for reasoning models. It can be set to "low", "medium", or "high".
	ReasoningEffort string `json:"reasoning_effort,omitempty" yaml:"reasoning_effort,omitempty"`

	Knowledge []*AssistantKnowledge `json:"knowledge,omitempty" yaml:"knowledge,omitempty"`

	IsActionableTemplate      string `json:"is_actionable_template,omitempty" yaml:"is_actionable_template,omitempty"`
	IsActionableHistoryLength int    `json:"is_actionable_history_length,omitempty" yaml:"is_actionable_history_length,omitempty"` // Defaults to 4

	APIs       []AssistantAPI       `json:"apis,omitempty" yaml:"apis,omitempty"`
	GPTScripts []AssistantGPTScript `json:"gptscripts,omitempty" yaml:"gptscripts,omitempty"`
	Zapier     []AssistantZapier    `json:"zapier,omitempty" yaml:"zapier,omitempty"`
	MCPs       []AssistantMCP       `json:"mcps,omitempty" yaml:"mcps,omitempty"`

	Browser   AssistantBrowser   `json:"browser,omitempty" yaml:"browser,omitempty"`
	WebSearch AssistantWebSearch `json:"web_search,omitempty" yaml:"web_search,omitempty"`

	Calculator  AssistantCalculator  `json:"calculator,omitempty" yaml:"calculator,omitempty"`
	Email       AssistantEmail       `json:"email,omitempty" yaml:"email,omitempty"`
	AzureDevOps AssistantAzureDevOps `json:"azure_devops,omitempty" yaml:"azure_devops,omitempty"`
	Tools       []*Tool              `json:"tools,omitempty" yaml:"tools,omitempty"`

	Tests []struct {
		Name  string     `json:"name,omitempty" yaml:"name,omitempty"`
		Steps []TestStep `json:"steps,omitempty" yaml:"steps,omitempty"`
	} `json:"tests,omitempty" yaml:"tests,omitempty"`
}

type AssistantBrowser struct {
	Enabled                bool `json:"enabled" yaml:"enabled"`
	MarkdownPostProcessing bool `json:"markdown_post_processing" yaml:"markdown_post_processing"` // If true, the browser will return the HTML as markdown
	ProcessOutput          bool `json:"process_output" yaml:"process_output"`                     // If true, the browser will process the output of the tool call before returning it to the top loop. Useful for skills that return structured data such as Browser,
	// TODO: whitelist URLs?
}

type AssistantWebSearch struct {
	Enabled    bool `json:"enabled" yaml:"enabled"`
	MaxResults int  `json:"max_results" yaml:"max_results"`
}

type AssistantCalculator struct {
	Enabled bool `json:"enabled" yaml:"enabled"`
}

// AssistantEmail - email sending tool, will use default email provider
// configured in helix
type AssistantEmail struct {
	Enabled         bool   `json:"enabled" yaml:"enabled"`
	TemplateExample string `json:"template_example" yaml:"template_example"`
}

const ReasoningEffortNone = "none" // Don't set

// Add this new type
type TestStep struct {
	Prompt         string `json:"prompt" yaml:"prompt"`
	ExpectedOutput string `json:"expected_output" yaml:"expected_output"`
}

type AppHelixConfig struct {
	Name              string            `json:"name,omitempty" yaml:"name,omitempty"`
	Description       string            `json:"description,omitempty" yaml:"description,omitempty"`
	Avatar            string            `json:"avatar,omitempty" yaml:"avatar,omitempty"`
	AvatarContentType string            `json:"avatar_content_type,omitempty" yaml:"avatar_content_type,omitempty"`
	Image             string            `json:"image,omitempty" yaml:"image,omitempty"`
	ExternalURL       string            `json:"external_url,omitempty" yaml:"external_url,omitempty"`
	Assistants        []AssistantConfig `json:"assistants,omitempty" yaml:"assistants,omitempty"`
	Triggers          []Trigger         `json:"triggers,omitempty" yaml:"triggers,omitempty"`
}

type AppHelixConfigMetadata struct {
	Name string `json:"name" yaml:"name"`
}

type AppHelixConfigCRD struct {
	APIVersion string                 `json:"apiVersion" yaml:"apiVersion"`
	Kind       string                 `json:"kind" yaml:"kind"`
	Metadata   AppHelixConfigMetadata `json:"metadata" yaml:"metadata"`
	Spec       AppHelixConfig         `json:"spec" yaml:"spec"`
}

type AppGithubConfigUpdate struct {
	Updated time.Time `json:"updated"`
	Hash    string    `json:"hash"`
	Error   string    `json:"error"`
}

type AppConfig struct {
	AllowedDomains []string          `json:"allowed_domains" yaml:"allowed_domains"`
	Secrets        map[string]string `json:"secrets" yaml:"secrets"`
	Helix          AppHelixConfig    `json:"helix" yaml:"helix"`
}

func (c AppConfig) Value() (driver.Value, error) {
	j, err := json.Marshal(c)
	return j, err
}

func (c *AppConfig) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New("type assertion .([]byte) failed")
	}
	var result AppConfig
	if err := json.Unmarshal(source, &result); err != nil {
		return err
	}
	*c = result
	return nil
}

func (AppConfig) GormDataType() string {
	return "json"
}

type DiscordTrigger struct {
	ServerName string `json:"server_name" yaml:"server_name"`
}

type SlackTrigger struct {
	Enabled  bool     `json:"enabled,omitempty"`
	AppToken string   `json:"app_token" yaml:"app_token"`
	BotToken string   `json:"bot_token" yaml:"bot_token"`
	Channels []string `json:"channels" yaml:"channels"`
}

// Crisp trigger configuration, create yours
// here https://marketplace.crisp.chat/plugins/
type CrispTrigger struct {
	Enabled    bool   `json:"enabled,omitempty"`
	Nickname   string `json:"nickname" yaml:"nickname"`     // Optional
	Identifier string `json:"identifier" yaml:"identifier"` // Token identifier
	Token      string `json:"token" yaml:"token"`
}

type CronTrigger struct {
	Enabled  bool   `json:"enabled,omitempty" yaml:"enabled,omitempty"`
	Schedule string `json:"schedule,omitempty" yaml:"schedule,omitempty"`
	Input    string `json:"input,omitempty" yaml:"input,omitempty"`
}

// AzureDevOpsTrigger - once enabled, a trigger in the database will be created
// that you can supply to Azure DevOps to trigger a session.
type AzureDevOpsTrigger struct {
	Enabled bool `json:"enabled,omitempty" yaml:"enabled,omitempty"`
}

type Trigger struct {
	Discord     *DiscordTrigger     `json:"discord,omitempty" yaml:"discord,omitempty"`
	Slack       *SlackTrigger       `json:"slack,omitempty" yaml:"slack,omitempty"`
	Cron        *CronTrigger        `json:"cron,omitempty" yaml:"cron,omitempty"`
	Crisp       *CrispTrigger       `json:"crisp,omitempty" yaml:"crisp,omitempty"`
	AzureDevOps *AzureDevOpsTrigger `json:"azure_devops,omitempty" yaml:"azure_devops,omitempty"`
}

func (t Trigger) Value() (driver.Value, error) {
	j, err := json.Marshal(t)
	return j, err
}

func (t *Trigger) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New("type assertion .([]byte) failed")
	}
	var result Trigger
	if err := json.Unmarshal(source, &result); err != nil {
		return err
	}
	*t = result
	return nil
}

func (Trigger) GormDataType() string {
	return "json"
}

type Triggers []Trigger

func (t Triggers) Value() (driver.Value, error) {
	j, err := json.Marshal(t)
	return j, err
}

func (t *Triggers) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New("type assertion .([]byte) failed")
	}
	var result []Trigger
	if err := json.Unmarshal(source, &result); err != nil {
		return err
	}
	*t = result
	return nil
}

func (Triggers) GormDataType() string {
	return "json"
}

type App struct {
	ID             string    `json:"id" gorm:"primaryKey"`
	Created        time.Time `json:"created"`
	Updated        time.Time `json:"updated"`
	OrganizationID string    `json:"organization_id" gorm:"index"`
	// uuid of user ID
	Owner string `json:"owner" gorm:"index"`
	// e.g. user, system, org
	OwnerType OwnerType `json:"owner_type"`
	Global    bool      `json:"global"`
	Config    AppConfig `json:"config" gorm:"jsonb"`

	User User `json:"user" gorm:"-"` // Owner user struct, populated by the server for organization views
}

type KeyPair struct {
	Type       string
	PrivateKey string
	PublicKey  string
}

// the low level "please run me a gptsript" request
type GptScript struct {
	// if the script is inline then we use loader.ProgramFromSource
	Source string `json:"source"`
	// if we have a file path then we use loader.Program
	// and gptscript will sort out relative paths
	// if this script is part of a github app
	// it will be a relative path inside the repo
	FilePath string `json:"file_path"`
	// if the script lives on a URL then we download it
	URL string `json:"url"`
	// the program inputs
	Input string `json:"input"`
	// this is the env passed into the program
	Env []string `json:"env"`
}

// higher level "run a script inside this repo" request
type GptScriptGithubApp struct {
	Script     GptScript `json:"script"`
	Repo       string    `json:"repo"`
	CommitHash string    `json:"commit"`
	// we will need this to clone the repo (in the case of private repos)
	KeyPair KeyPair `json:"key_pair"`
}

// for an app, run which script with what input?
type GptScriptRequest struct {
	FilePath string `json:"file_path"`
	Input    string `json:"input"`
}

type GptScriptResponse struct {
	Output  string `json:"output"`
	Error   string `json:"error"`
	Retries int    `json:"retries"`
}

func (g GptScriptResponse) Value() (driver.Value, error) {
	j, err := json.Marshal(g)
	return j, err
}

func (g *GptScriptResponse) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New("type assertion .([]byte) failed")
	}
	var result GptScriptResponse
	if err := json.Unmarshal(source, &result); err != nil {
		return err
	}
	*g = result
	return nil
}

func (GptScriptResponse) GormDataType() string {
	return "json"
}

type DataEntityConfig struct {
	FilestorePath string      `json:"filestore_path"`
	RAGSettings   RAGSettings `json:"rag_settings"`
}

func (d DataEntityConfig) Value() (driver.Value, error) {
	j, err := json.Marshal(d)
	return j, err
}

func (d *DataEntityConfig) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New("type assertion .([]byte) failed")
	}
	var result DataEntityConfig
	if err := json.Unmarshal(source, &result); err != nil {
		return err
	}
	*d = result
	return nil
}

func (DataEntityConfig) GormDataType() string {
	return "json"
}

type DataEntity struct {
	ID      string         `json:"id" gorm:"primaryKey"`
	Created time.Time      `json:"created"`
	Updated time.Time      `json:"updated"`
	Name    string         `json:"name"`
	Type    DataEntityType `json:"type"`
	// uuid of owner entity
	Owner string `json:"owner" gorm:"index"`
	// e.g. user, system, org
	OwnerType OwnerType `json:"owner_type"`
	// some datasets are parents to others for example
	// a folder of files can be the source of a RAG dataset
	// or a qapairs dataset - a qapairs dataset can be the source
	// of a lora dataset.
	ParentDataEntity string           `json:"parent_entity"`
	Config           DataEntityConfig `json:"config" gorm:"jsonb"`
}

type ScriptRunType string

const (
	GptScriptRunnerTaskTypeGithubApp ScriptRunType = "github_app"
	GptScriptRunnerTaskTypeTool      ScriptRunType = "tool"
	// TODO: add more types, like python script, etc.
)

// ScriptRun is an internal type that is used when GPTScript
// tasks are invoked by the user and the runner runs
type ScriptRun struct {
	ID         string         `json:"id" gorm:"primaryKey"`
	Created    time.Time      `json:"created"`
	Updated    time.Time      `json:"updated"`
	Owner      string         `json:"owner" gorm:"index"` // uuid of owner entity
	OwnerType  OwnerType      `json:"owner_type"`         // e.g. user, system, org
	AppID      string         `json:"app_id"`
	State      ScriptRunState `json:"state"`
	Type       ScriptRunType  `json:"type"`
	Retries    int            `json:"retries"`
	DurationMs int            `json:"duration_ms"`

	Request     *GptScriptRunnerRequest `json:"request" gorm:"jsonb"`
	Response    *GptScriptResponse      `json:"response" gorm:"jsonb"`
	SystemError string                  `json:"system_error"` // If we didn't get the response from the runner
}

type GptScriptRunsQuery struct {
	Owner     string
	OwnerType OwnerType
	AppID     string
	State     ScriptRunState
}

type GptScriptRunnerRequest struct {
	GithubApp *GptScriptGithubApp `json:"github_app"`
}

func (r GptScriptRunnerRequest) Value() (driver.Value, error) {
	j, err := json.Marshal(r)
	return j, err
}

func (r *GptScriptRunnerRequest) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New("type assertion .([]byte) failed")
	}
	var result GptScriptRunnerRequest
	if err := json.Unmarshal(source, &result); err != nil {
		return err
	}
	*r = result
	return nil
}

func (GptScriptRunnerRequest) GormDataType() string {
	return "json"
}

type RunnerEventRequestType int

func (r RunnerEventRequestType) String() string {
	switch r {
	case RunnerEventRequestTool:
		return "tool"
	case RunnerEventRequestApp:
		return "app"
	default:
		return "unknown"
	}
}

const (
	RunnerEventRequestTool RunnerEventRequestType = iota
	RunnerEventRequestApp
)

type RunnerEventRequestEnvelope struct {
	RequestID string                 `json:"request_id"`
	Payload   []byte                 `json:"payload"`
	Type      RunnerEventRequestType `json:"type"`
	Reply     string                 `json:"reply"` // Where to send the reply
}

type RunnerEventResponseEnvelope struct {
	RequestID string `json:"request_id"`
	Reply     string `json:"reply"` // Where to send the reply
	Payload   []byte `json:"payload"`
}

type RunnerLLMInferenceRequest struct {
	// RequestID is generated when a new request
	// is received on the internal Helix OpenAI client
	// to generate a chat completions call
	RequestID string

	CreatedAt time.Time

	Priority      bool
	OwnerID       string
	SessionID     string
	InteractionID string

	Request *openai.ChatCompletionRequest

	// Added fields for embeddings
	Embeddings               bool                      `json:"embeddings,omitempty"`
	EmbeddingRequest         openai.EmbeddingRequest   `json:"embedding_request,omitempty"`
	FlexibleEmbeddingRequest *FlexibleEmbeddingRequest `json:"flexible_embedding_request,omitempty"`
}

type RunnerLLMInferenceResponse struct {
	// RequestID is used to match the response
	// to the request
	RequestID string

	OwnerID       string
	SessionID     string
	InteractionID string

	Response       *openai.ChatCompletionResponse
	StreamResponse *openai.ChatCompletionStreamResponse

	// Error is set if there was an error
	Error string

	DurationMs int64

	Done bool
}

// LLMCallStep used to categorize LLM call steps
// where it's applicable
type LLMCallStep string

const (
	LLMCallStepDefault               LLMCallStep = "default"
	LLMCallStepIsActionable          LLMCallStep = "is_actionable"
	LLMCallStepPrepareAPIRequest     LLMCallStep = "prepare_api_request"
	LLMCallStepInterpretResponse     LLMCallStep = "interpret_response"
	LLMCallStepGenerateTitle         LLMCallStep = "generate_title"
	LLMCallStepSummarizeConversation LLMCallStep = "summarize_conversation"
)

// LLMCall used to store the request and response of LLM calls
// done by helix to LLM providers such as openai, togetherai or helix itself
type LLMCall struct {
	ID               string         `json:"id" gorm:"primaryKey"`
	AppID            string         `json:"app_id" gorm:"index:idx_app_interaction,priority:1"`
	OrganizationID   string         `json:"organization_id" gorm:"index"`
	UserID           string         `json:"user_id" gorm:"index"`
	Created          time.Time      `json:"created"`
	Updated          time.Time      `json:"updated"`
	SessionID        string         `json:"session_id" gorm:"index"`
	InteractionID    string         `json:"interaction_id" gorm:"index:idx_app_interaction,priority:2"`
	Model            string         `json:"model"`
	Provider         string         `json:"provider"`
	Step             LLMCallStep    `json:"step" gorm:"index"`
	OriginalRequest  datatypes.JSON `json:"original_request" gorm:"type:jsonb"`
	Request          datatypes.JSON `json:"request" gorm:"type:jsonb"`
	Response         datatypes.JSON `json:"response" gorm:"type:jsonb"`
	DurationMs       int64          `json:"duration_ms"`
	PromptTokens     int64          `json:"prompt_tokens"`
	CompletionTokens int64          `json:"completion_tokens"`
	TotalTokens      int64          `json:"total_tokens"`
	PromptCost       float64        `json:"prompt_cost"`
	CompletionCost   float64        `json:"completion_cost"`
	TotalCost        float64        `json:"total_cost"` // Total cost of the call (prompt and completion tokens)
	Stream           bool           `json:"stream"`
	Error            string         `json:"error"`
}

type CreateSecretRequest struct {
	Name  string `json:"name"`
	Value string `json:"value"`
	AppID string `json:"app_id"`
}

type Secret struct {
	ID        string    `json:"id,omitempty" yaml:"id,omitempty"`
	Created   time.Time `json:"created,omitempty" yaml:"created,omitempty"`
	Updated   time.Time `json:"updated,omitempty" yaml:"updated,omitempty"`
	Owner     string
	OwnerType OwnerType
	Name      string `json:"name" yaml:"name"`
	Value     []byte `json:"value" yaml:"value" gorm:"type:bytea"`
	AppID     string `json:"app_id" yaml:"app_id"` // optional, if set, the secret will be available to the specified app
}

// LicenseKey represents a license key in the database
type LicenseKey struct {
	ID         uint      `gorm:"primarykey" json:"id"`
	LicenseKey string    `json:"license_key"`
	CreatedAt  time.Time `json:"created_at"`
	UpdatedAt  time.Time `json:"updated_at"`
}

type GetDesiredRunnerSlotsResponse struct {
	Data []DesiredRunnerSlot `json:"data"`
}

type DesiredSlots struct {
	ID   string              `json:"id"`
	Data []DesiredRunnerSlot `json:"data"`
}

type DesiredRunnerSlot struct {
	ID         uuid.UUID                   `json:"id"`
	Attributes DesiredRunnerSlotAttributes `json:"attributes"`
}

type WorkloadType string

const (
	WorkloadTypeLLMInferenceRequest WorkloadType = "llm"
	WorkloadTypeSession             WorkloadType = "session"
)

type DesiredRunnerSlotAttributes struct {
	Workload *RunnerWorkload `json:"workload,omitempty"`
	Model    string          `json:"model"`
	Mode     string          `json:"mode"`
}

type RunnerWorkload struct {
	LLMInferenceRequest *RunnerLLMInferenceRequest
	Session             *Session
}

type RunnerActualSlot struct {
	ID         uuid.UUID                  `json:"id"`
	Attributes RunnerActualSlotAttributes `json:"attributes"`
}

type RunnerActualSlotAttributes struct {
	OriginalWorkload *RunnerWorkload `json:"original_workload,omitempty"`
	CurrentWorkload  *RunnerWorkload `json:"current_workload,omitempty"`
	RunnerSlot       *RunnerSlot     `json:"runner_slot,omitempty"`
}

type RunAPIActionRequest struct {
	Action     string                 `json:"action"`
	Parameters map[string]interface{} `json:"parameters"`

	Tool *Tool `json:"-"` // Set internally

	OAuthTokens map[string]string `json:"-"` // OAuth tokens mapped by provider type (lowercase)
}

type RunAPIActionResponse struct {
	Response string `json:"response"` // Raw response from the API
	Error    string `json:"error"`
}

type RunnerAttributes struct {
	TotalMemory uint64       `json:"total_memory"`
	FreeMemory  uint64       `json:"free_memory"`
	Version     string       `json:"version"`
	Slots       []RunnerSlot `json:"slots"`
}

type Runner struct {
	ID         string           `json:"id"`
	Attributes RunnerAttributes `json:"attributes"`
}

type GetRunnersResponse struct {
	Runners []Runner `json:"runners"`
}

// Add this struct to represent the license info we want to send to the frontend
type FrontendLicenseInfo struct {
	Valid        bool      `json:"valid"`
	Organization string    `json:"organization"`
	ValidUntil   time.Time `json:"valid_until"`
	Features     struct {
		Users bool `json:"users"`
	} `json:"features"`
	Limits struct {
		Users    int64 `json:"users"`
		Machines int64 `json:"machines"`
	} `json:"limits"`
}

type LoginRequest struct {
	RedirectURI string `json:"redirect_uri"`
}

type UserResponse struct {
	ID    string `json:"id"`
	Email string `json:"email"`
	Token string `json:"token"`
	Name  string `json:"name"`
}

type AuthenticatedResponse struct {
	Authenticated bool `json:"authenticated"`
}

type TokenResponse struct {
	Token string `json:"token"`
}

// This response represents what can be done with the context menu. The goal is to provide a list of
// actions that can be taken, like "filter" or "include".
// The UI is designed so that it groups each action into a section. If you want a multi-level menu,
// you will need to implement that.
type ContextMenuResponse struct {
	Data []ContextMenuAction `json:"data"`
}
type ContextMenuAction struct {
	ActionLabel string `json:"action_label"` // Forms the grouping in the UI
	Label       string `json:"label"`        // The label that will be shown in the UI
	Value       string `json:"value"`        // The value written to the text area when the action is selected
}

type UsageMetric struct {
	ID                string    `json:"id" gorm:"primaryKey"`
	Created           time.Time `json:"created" gorm:"index:idx_app_time,priority:2"`
	Date              time.Time `json:"date" gorm:"index:idx_app_time,priority:1"` // The date of the metric (without time, just the date)
	AppID             string    `json:"app_id" gorm:"index:idx_app_time,priority:1"`
	OrganizationID    string    `json:"organization_id"`
	InteractionID     string    `json:"interaction_id"`
	UserID            string    `json:"user_id"`
	Provider          string    `json:"provider"`
	Model             string    `json:"model"`
	PromptTokens      int       `json:"prompt_tokens"`
	CompletionTokens  int       `json:"completion_tokens"`
	TotalTokens       int       `json:"total_tokens"`
	PromptCost        float64   `json:"prompt_cost"`
	CompletionCost    float64   `json:"completion_cost"`
	TotalCost         float64   `json:"total_cost"` // Total cost of the call (prompt and completion tokens)
	DurationMs        int       `json:"duration_ms"`
	RequestSizeBytes  int       `json:"request_size_bytes"`
	ResponseSizeBytes int       `json:"response_size_bytes"`
}

type UsersAggregatedUsageMetric struct {
	User    User                    `json:"user"`
	Metrics []AggregatedUsageMetric `json:"metrics"`
}

type AggregatedUsageMetric struct {
	// ID    string    `json:"id" gorm:"primaryKey"`
	Date time.Time `json:"date"` // The date of the metric (without time, just the date)

	PromptTokens      int     `json:"prompt_tokens"`
	CompletionTokens  int     `json:"completion_tokens"`
	TotalTokens       int     `json:"total_tokens"`
	PromptCost        float64 `json:"prompt_cost"`
	CompletionCost    float64 `json:"completion_cost"`
	TotalCost         float64 `json:"total_cost"` // Total cost of the call (prompt and completion tokens)
	LatencyMs         float64 `json:"latency_ms"`
	RequestSizeBytes  int     `json:"request_size_bytes"`
	ResponseSizeBytes int     `json:"response_size_bytes"`
	TotalRequests     int     `json:"total_requests"`
}

// Response for the user access endpoint
type UserAppAccessResponse struct {
	CanRead  bool `json:"can_read"`
	CanWrite bool `json:"can_write"`
	IsAdmin  bool `json:"is_admin"`
}

// TextPart represents a text content part for explicit typing if needed elsewhere.
type TextPart struct {
	Type string `json:"type"` // Expected to be "text"
	Text string `json:"text"`
}

// ImageURLData represents the data for an image URL.
type ImageURLData struct {
	URL    string `json:"url"`
	Detail string `json:"detail,omitempty"` // e.g., "auto", "low", "high"
}

// ImageURLPart represents an image URL content part for explicit typing if needed elsewhere.
type ImageURLPart struct {
	Type     string       `json:"type"` // Expected to be "image_url"
	ImageURL ImageURLData `json:"image_url"`
}

// FlexibleEmbeddingRequest represents a flexible embedding request that can handle both
// standard OpenAI embedding format and Chat Embeddings API format with messages
type FlexibleEmbeddingRequest struct {
	Model          string                  `json:"model"`
	Input          interface{}             `json:"input,omitempty"`    // Can be string, []string, [][]int, etc.
	Messages       []ChatCompletionMessage `json:"messages,omitempty"` // For Chat Embeddings API format
	EncodingFormat string                  `json:"encoding_format,omitempty"`
	Dimensions     int                     `json:"dimensions,omitempty"`
}

// FlexibleEmbeddingResponse represents a flexible embedding response
type FlexibleEmbeddingResponse struct {
	Object string `json:"object"`
	Data   []struct {
		Object    string    `json:"object"`
		Index     int       `json:"index"`
		Embedding []float32 `json:"embedding"`
	} `json:"data"`
	Model string `json:"model"`
	Usage struct {
		PromptTokens int `json:"prompt_tokens"`
		TotalTokens  int `json:"total_tokens"`
	} `json:"usage"`
}

type SchedulingDecisionType string

const (
	SchedulingDecisionTypeQueued         SchedulingDecisionType = "queued"           // Added to queue
	SchedulingDecisionTypeReuseWarmSlot  SchedulingDecisionType = "reuse_warm_slot"  // Reused existing warm model instance
	SchedulingDecisionTypeCreateNewSlot  SchedulingDecisionType = "create_new_slot"  // Started new model instance
	SchedulingDecisionTypeEvictStaleSlot SchedulingDecisionType = "evict_stale_slot" // Evicted stale slot to free memory
	SchedulingDecisionTypeRejected       SchedulingDecisionType = "rejected"         // Rejected (insufficient resources, etc.)
	SchedulingDecisionTypeError          SchedulingDecisionType = "error"            // Error during scheduling
	SchedulingDecisionTypeUnschedulable  SchedulingDecisionType = "unschedulable"    // Cannot be scheduled (no warm slots available)
)

// SchedulingDecision represents a decision made by the central scheduler
type SchedulingDecision struct {
	ID               string                 `json:"id"`
	Created          time.Time              `json:"created"`
	WorkloadID       string                 `json:"workload_id"`
	SessionID        string                 `json:"session_id"`
	ModelName        string                 `json:"model_name"`
	Mode             SessionMode            `json:"mode"`
	DecisionType     SchedulingDecisionType `json:"decision_type"`
	RunnerID         string                 `json:"runner_id,omitempty"`
	SlotID           string                 `json:"slot_id,omitempty"`
	Reason           string                 `json:"reason"`
	Success          bool                   `json:"success"`
	ProcessingTimeMs int64                  `json:"processing_time_ms"`
	QueuePosition    int                    `json:"queue_position,omitempty"`
	AvailableRunners []string               `json:"available_runners,omitempty"`
	MemoryRequired   uint64                 `json:"memory_required,omitempty"`
	MemoryAvailable  uint64                 `json:"memory_available,omitempty"`
	WarmSlotCount    int                    `json:"warm_slot_count,omitempty"`
	TotalSlotCount   int                    `json:"total_slot_count,omitempty"`
	RepeatCount      int                    `json:"repeat_count,omitempty"`
}

// SlackThread used to track the state of slack threads where Helix agent is invoked
type SlackThread struct {
	ThreadKey string    `json:"thread_key" gorm:"primaryKey"`
	AppID     string    `json:"app_id" gorm:"primaryKey"`
	Channel   string    `json:"channel" gorm:"primaryKey"`
	Created   time.Time `json:"created"`
	Updated   time.Time `json:"updated"`

	SessionID string `json:"session_id"`
}

type CrispThread struct {
	CrispSessionID string    `json:"crisp_session_id" gorm:"primaryKey"`
	AppID          string    `json:"app_id" gorm:"primaryKey"`
	Created        time.Time `json:"created"`
	Updated        time.Time `json:"updated"`

	SessionID string `json:"session_id"` // Helix session ID
}

type TriggerType string

func (t TriggerType) String() string {
	return string(t)
}

const (
	TriggerTypeSlack       TriggerType = "slack"
	TriggerTypeCrisp       TriggerType = "crisp"
	TriggerTypeAzureDevOps TriggerType = "azure_devops"
	TriggerTypeCron        TriggerType = "cron"
	// TODO: discord
)

// TriggerStatus is used to provide trigger status
// to the frontend (discord, slack bots, etc)
type TriggerStatus struct {
	Type    TriggerType `json:"type"`
	OK      bool        `json:"ok"`
	Message string      `json:"message"`
}

type TriggerConfiguration struct {
	ID             string      `json:"id"`
	Created        time.Time   `json:"created"`
	Updated        time.Time   `json:"updated"`
	Archived       bool        `json:"archived"`
	Enabled        bool        `json:"enabled"`
	AppID          string      `json:"app_id"`          // App ID
	OrganizationID string      `json:"organization_id"` // Organization ID
	Owner          string      `json:"owner"`           // User ID
	OwnerType      OwnerType   `json:"owner_type"`      // User or Organization
	Name           string      `json:"name"`            // Name of the trigger configuration
	Trigger        Trigger     `json:"trigger" gorm:"jsonb"`
	TriggerType    TriggerType `json:"trigger_type"`

	WebhookURL string `json:"webhook_url" gorm:"-"` // Webhook URL for the trigger configuration, applicable to webhook type triggers like Azure DevOps, GitHub, etc.

	OK     bool   `json:"ok" gorm:"-"`
	Status string `json:"status" gorm:"-"`
}

type TriggerExecuteResponse struct {
	SessionID string `json:"session_id"`
	Content   string `json:"content"`
}

type TriggerExecutionStatus string

const (
	TriggerExecutionStatusPending TriggerExecutionStatus = "pending"
	TriggerExecutionStatusRunning TriggerExecutionStatus = "running"
	TriggerExecutionStatusSuccess TriggerExecutionStatus = "success"
	TriggerExecutionStatusError   TriggerExecutionStatus = "error"
)

type TriggerExecution struct {
	ID                     string                 `json:"id"`
	Created                time.Time              `json:"created"`
	Updated                time.Time              `json:"updated"`
	TriggerConfigurationID string                 `json:"trigger_configuration_id"`
	Name                   string                 `json:"name"` // Will most likely match session name, based on the trigger name at the time of execution
	DurationMs             int64                  `json:"duration_ms"`
	Status                 TriggerExecutionStatus `json:"status"`
	Error                  string                 `json:"error"`
	Output                 string                 `json:"output"`
	SessionID              string                 `json:"session_id"`
}

// Memory provides agent user memories
type Memory struct {
	ID       string    `json:"id"`
	Created  time.Time `json:"created"`
	Updated  time.Time `json:"updated"`
	UserID   string    `json:"user_id"`
	AppID    string    `json:"app_id"`
	Contents string    `json:"contents"`
}

type ListMemoryRequest struct {
	UserID string
	AppID  string
}
