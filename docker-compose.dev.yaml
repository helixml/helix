# Common runner configuration (YAML anchor, not a service)
x-runner-config: &runner-config
    restart: always
    entrypoint: bash -c "cd /workspace/helix && go version && tail -f /dev/null"
    env_file:
        - .env
    environment:
        # Explicitly set Go cache paths for clarity
        - GOMODCACHE=/go/pkg/mod
        - GOCACHE=/root/.cache/go-build
        # Prevent automatic toolchain downloads
        - GOTOOLCHAIN=local
    volumes:
        - .:/workspace/helix
        - ~/.cache/huggingface:/root/.cache/huggingface
        # Go caches for faster development
        - go-pkg-mod:/go/pkg/mod
        - go-build-cache:/root/.cache/go-build
        - go-sdk-cache:/root/.cache/go
        # comment these out if you don't have appropriate repos checked out
        # - ../axolotl:/workspace/axolotl

services:
    api:
        build:
            context: .
            dockerfile: Dockerfile
            target: api-dev-env
        networks:
            default:
                ipv4_address: 172.19.0.20
        ports:
            - ${API_PORT:-8080}:8080
            - "3478:3478/udp"  # TURN server for WebRTC NAT traversal
            - "3478:3478/tcp"  # TURN server TCP transport (firewall fallback)
        restart: always
        env_file:
            - .env
        environment:
            - SERVER_PORT=8080
            - LOG_LEVEL=${LOG_LEVEL:-debug}
            - POSTGRES_HOST=postgres
            - POSTGRES_DATABASE=postgres
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=${POSTGRES_ADMIN_PASSWORD-postgres}
            - RUNNER_TOKEN=${RUNNER_TOKEN-oh-hallo-insecure-token}
            - SERVER_URL=${SERVER_URL:-http://localhost:8080}
            - KEYCLOAK_URL=http://keycloak:8080/auth
            - JANITOR_SLACK_WEBHOOK_URL=${JANITOR_SLACK_WEBHOOK_URL:-}
            - JANITOR_SLACK_IGNORE_USERS=${JANITOR_SLACK_IGNORE_USERS:-}
            - OPENAI_API_KEY=${OPENAI_API_KEY:-}
            - TOGETHER_API_KEY=${TOGETHER_API_KEY:-}
            - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
            - HF_TOKEN=${HF_TOKEN:-}
            - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY:-}
            - STRIPE_WEBHOOK_SIGNING_SECRET=${STRIPE_WEBHOOK_SIGNING_SECRET:-}
            - STRIPE_PRICE_LOOKUP_KEY=${STRIPE_PRICE_LOOKUP_KEY:-}
            - FRONTEND_URL=http://frontend:8081
            # this is an insecure development key do not use!
            - KEYCLOAK_USER=admin
            - KEYCLOAK_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD-oh-hallo-insecure-password}
            - KEYCLOAK_FRONTEND_URL=${KEYCLOAK_FRONTEND_URL:-http://localhost:8080/auth/}
            # lock down dashboard in production
            - ADMIN_USER_IDS=${ADMIN_USER_IDS-all}
            - EVAL_USER_ID=${EVAL_USER_ID:-}
            - FILESTORE_LOCALFS_PATH=/filestore
            - SENTRY_DSN_API=${SENTRY_DSN_API:-}
            - SENTRY_DSN_FRONTEND=${SENTRY_DSN_FRONTEND:-}
            - GOOGLE_ANALYTICS_FRONTEND=${GOOGLE_ANALYTICS_FRONTEND:-}
            # Tools configuration
            - TOOLS_ENABLED=true
            - TOOLS_PROVIDER=${TOOLS_PROVIDER:-helix}
            - TOOLS_MODEL=${TOOLS_MODEL:-llama3:instruct}
            # Email notifications
            - EMAIL_MAILGUN_DOMAIN=${EMAIL_MAILGUN_DOMAIN:-}
            - EMAIL_MAILGUN_API_KEY=${EMAIL_MAILGUN_API_KEY:-}
            # SMTP
            - EMAIL_SMTP_HOST=${EMAIL_SMTP_HOST:-}
            - EMAIL_SMTP_PORT=${EMAIL_SMTP_PORT:-}
            - EMAIL_SMTP_USERNAME=${EMAIL_SMTP_USERNAME:-}
            - EMAIL_SMTP_PASSWORD=${EMAIL_SMTP_PASSWORD:-}
            # Discord integration
            - DISCORD_BOT_TOKEN=${DISCORD_BOT_TOKEN:-}
            - ADMIN_USER_SOURCE=${ADMIN_USER_SOURCE:-}
            # AI Providers management - enable/disable user-facing AI provider API keys management
            - PROVIDERS_MANAGEMENT_ENABLED=${PROVIDERS_MANAGEMENT_ENABLED:-true}
            # Socket configuration for haystack communication
            - HELIX_EMBEDDINGS_SOCKET=/socket/embeddings.sock
            - RAG_HAYSTACK_ENABLED=true
            - RAG_DEFAULT_PROVIDER=${RAG_DEFAULT_PROVIDER:-haystack}
            # URL in the compose stack (rather than localhost in the pod which is default for k8s)
            - RAG_HAYSTACK_URL=http://haystack:8000
            - RAG_PGVECTOR_PROVIDER=helix
            # Dynamic providers: format is "provider1:api_key1:base_url1,provider2:api_key2:base_url2"
            - DYNAMIC_PROVIDERS=${DYNAMIC_PROVIDERS:-}
            # Wolf integration for external agents
            - WOLF_SOCKET_PATH=/var/run/wolf/wolf.sock
            - WOLF_MODE=${WOLF_MODE:-apps}  # "apps" (default) or "lobbies" (multi-user with PIN support)
            - ZED_IMAGE=${ZED_IMAGE:-helix-sway:latest}
            - HELIX_HOST_HOME=${HELIX_HOST_HOME}
            - HELIX_DEV_MODE=true  # Enable dev file bind-mounts for hot-reloading
            # Moonlight-web architecture mode: "single" (keepalive/join, session-persistence) or "multi" (streamers API, broadcasting)
            - MOONLIGHT_WEB_MODE=${MOONLIGHT_WEB_MODE:-single}
            - MOONLIGHT_CREDENTIALS=${MOONLIGHT_CREDENTIALS:-helix}
            # TURN server configuration for WebRTC
            - TURN_ENABLED=true
            - TURN_PUBLIC_IP=${TURN_PUBLIC_IP:-api}  # Use 'api' hostname for local relay (auto-detects public IP)
            - TURN_PORT=3478
            - TURN_REALM=helix.ai
            - TURN_USERNAME=helix
            - TURN_PASSWORD=${TURN_PASSWORD:-helix-turn-secret}
        volumes:
            - ./go.mod:/app/go.mod
            - ./go.sum:/app/go.sum
            - ./api:/app/api
            - ./WORKDIR_README.md:/opt/helix/WORKDIR_README.md:ro
            - ${FILESTORE_DATA:-helix-filestore}:/filestore
            - helix-socket:/socket
            # Bind-mount from host (Wolf creates socket here)
            - /var/run/wolf:/var/run/wolf:rw
        depends_on:
            - postgres
            - keycloak
            - postgres-mcp
        extra_hosts:
            - "host.docker.internal:host-gateway"
    postgres:
        image: postgres:12.13-alpine
        restart: always
        # ports:
        #  - 5432:5432
        volumes:
            - ${POSTGRES_DATA:-helix-postgres-db}:/var/lib/postgresql/data
            - ./scripts/postgres:/docker-entrypoint-initdb.d

        environment:
            - POSTGRES_DB=postgres
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=${POSTGRES_ADMIN_PASSWORD-postgres}
            - POSTGRES_DATABASES=keycloak
    # Example/test MCP server
    postgres-mcp:
        image: crystaldba/postgres-mcp:latest
        restart: always
        command: ["--access-mode=unrestricted", "--transport=sse"]
        environment:
            - DATABASE_URI=postgresql://postgres:postgres@postgres:5432/postgres
        ports:
        - 8000:8000
    # postgres 15 with pgvector installed
    # why run this as a different server?
    # because we want the quick path to something working without having to create a hard dependency on pgvector
    # being installed in our main database
    # also - we would need to migrate our existing postgres 12 DB -> 17, which is a bit of a pain
    # TODO: figure out how to ship the pgvector extension with our main database
    # so we don't need to run what is essentially 2 versions of postgres
    pgvector:
        image: ghcr.io/tensorchord/vchord_bm25-postgres:pg17-v0.1.1
        restart: always
        # ports:
        #  - 5433:5432
        volumes:
            - ${PGVECTOR_DATA:-helix-pgvector-db}:/var/lib/postgresql/data
        environment:
            - POSTGRES_DB=postgres
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=${PGVECTOR_PASSWORD-postgres}
    keycloak:
        image: quay.io/keycloak/keycloak:23.0
        restart: always
        environment:
            - KC_DB=postgres
            - KC_DB_URL=jdbc:postgresql://postgres:5432/keycloak
            - KC_DB_USERNAME=postgres
            - KC_DB_PASSWORD=${POSTGRES_ADMIN_PASSWORD-postgres}
            - KEYCLOAK_ADMIN=admin
            - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD-oh-hallo-insecure-password}
            - KC_PROXY_HEADERS=forwarded|xforwarded
            - KC_HEALTH_ENABLED=true
            - KC_HOSTNAME_PATH=/auth
            - KC_HTTP_RELATIVE_PATH=/auth/
            - KC_HOSTNAME_URL=${KEYCLOAK_FRONTEND_URL:-http://localhost:8080/auth/}
            - KC_HOSTNAME_ADMIN_URL=${KEYCLOAK_FRONTEND_URL:-http://localhost:8080/auth/}
            # Theme development settings - disable caching
            - KC_SPI_THEME_CACHE_THEMES=false
            - KC_SPI_THEME_CACHE_TEMPLATES=false
            - KC_SPI_THEME_STATIC_MAX_AGE=-1
        volumes:
            # Mount themes directory for development
            - ./themes:/opt/keycloak/themes
        healthcheck:
            test:
                [
                    "CMD-SHELL",
                    "exec 3<>/dev/tcp/127.0.0.1/8080;echo -e \"GET /auth/health/ready HTTP/1.1\r\nhost: http://localhost\r\nConnection: close\r\n\r\n\" >&3;grep \"HTTP/1.1 200 OK\" <&3",
                ]
            interval: 5s
            timeout: 5s
            retries: 30
        command: ["start-dev"]
        # ports:
        #   - 30080:8080
    webhook_relay_stripe:
        image: webhookrelay/webhookrelayd
        environment:
            - KEY=${WEBHOOK_RELAY_KEY:-}
            - SECRET=${WEBHOOK_RELAY_SECRET:-}
            - BUCKET=${WEBHOOK_RELAY_BUCKET:-}
    tika:
        image: apache/tika:2.9.2.1
        # ports:
        #   - 9998:9998
    searxng:
        image: searxng/searxng:latest
        # ports:
        #   - 8112:8080
        volumes:
            - ./searxng/settings.yml:/etc/searxng/settings.yml
            - ./searxng/limiter.toml:/etc/searxng/limiter.toml
        environment:
            - BASE_URL=http://searxng:8080
            - INSTANCE_NAME=helix-instance
            - UWSGI_WORKERS=4
            - UWSGI_THREADS=4

    typense-ui:
        image: ghcr.io/bfritscher/typesense-dashboard:latest
        # ports:
        #   - 8877:80
        volumes:
            - ./scripts/config.json:/srv/config.json
        depends_on:
            - typesense
    typesense:
        build:
            context: .
            dockerfile: Dockerfile.typesense
        command: ["--data-dir", "/data", "--api-key", "typesense"]
        # ports:
        #   - 8108:8108
        volumes:
            - ${TYPESENSE_DATA:-helix-typesense-db}:/data
    chrome:
        image: ghcr.io/go-rod/rod:v0.115.0
        restart: always
        volumes:
            - ./integration-test/data/smoke:/integration-test/data/smoke
        # ports:
        #   - 7317:7317

    wolf:
      image: wolf:helix-fixed
      # image: ghcr.io/games-on-whales/wolf:wolf-ui
      networks:
        default:
          ipv4_address: 172.19.0.50
      environment:
        - NVIDIA_DRIVER_CAPABILITIES=all
        - NVIDIA_VISIBLE_DEVICES=all
        # Wolf-UI required environment variables
        - XDG_RUNTIME_DIR=/tmp/sockets
        - HOST_APPS_STATE_FOLDER=/etc/wolf
        - WOLF_SOCKET_PATH=/var/run/wolf/wolf.sock
        # Debug logging for HTTPS connection issues and UDP/RTP activity
        - WOLF_LOG_LEVEL=DEBUG
        # GST_DEBUG levels: 0=none, 1=ERROR, 2=WARNING, 3=INFO, 4=DEBUG, 5=LOG (full trace)
        # Level 3 (INFO) temporarily enabled for buffer pool diagnosis
        # WARNING: Level 5 produces massive logs - use sparingly
        - GST_DEBUG=3
        # NOTE: Interpipe cleanup CRITICAL errors are harmless - occur when containers terminate
        # "gst_inter_pipe_ilistener_push_buffer: assertion failed" during session cleanup
        # Fix HTTPS serverinfo hangs by avoiding slow MAC address enumeration
        - WOLF_INTERNAL_MAC=00:11:22:33:44:55
        - WOLF_INTERNAL_IP=172.19.0.50
        # Fix HTTPS SSL certificate paths for proper SSL operation
        - WOLF_PRIVATE_KEY_FILE=/etc/wolf/cfg/key.pem
        - WOLF_PRIVATE_CERT_FILE=/etc/wolf/cfg/cert.pem
        # Enable Nvidia zero-copy for better performance with new waylanddisplaysrc
        # New wolf-ui branch supports direct CUDAMemory output from waylanddisplaysrc
        - WOLF_USE_ZERO_COPY=TRUE
        # Internal moonlight-web pairing PIN (secure random, shared via env)
        - MOONLIGHT_INTERNAL_PAIRING_PIN=${MOONLIGHT_INTERNAL_PAIRING_PIN:-}
      volumes:
        - ./wolf/:/etc/wolf/cfg/
        # Mount init script into container init directory
        - ./wolf/init-wolf-config.sh:/etc/cont-init.d/05-init-wolf-config.sh:ro
        - /tmp/sockets:/tmp/sockets:rw
        - /var/run/docker.sock:/var/run/docker.sock:rw
        - /dev/:/dev/:rw
        - /run/udev:/run/udev:rw
        # Bind-mount to host so wolf-ui containers can bind-mount it
        - /var/run/wolf:/var/run/wolf:rw
      device_cgroup_rules:
        - 'c 13:* rmw'
      devices:
        - /dev/dri
        - /dev/uinput
        - /dev/uhid
      ports:
        # Moonlight protocol ports
        - "47984:47984"     # HTTPS
        - "47989:47989"     # HTTP (pairing, serverinfo)
        - "48010:48010"     # RTSP
        - "47415:47415/udp" # Discovery
        - "47999:47999/udp" # Control
        - "48100:48100/udp" # Video RTP
        - "48200:48200/udp" # Audio RTP
      runtime: nvidia
      deploy:
        resources:
          reservations:
            devices:
              - capabilities: [gpu]
      restart: unless-stopped

    # Moonlight Web Stream - Browser-based Moonlight streaming
    moonlight-web:
      image: helix-moonlight-web:helix-fixed
      command: bash -c "/app/server/init-moonlight-config.sh"
      ports:
        - "8081:8080"  # Web interface
        - "40000-40100:40000-40100/udp"  # WebRTC UDP ports (expanded for multiple concurrent sessions)
      networks:
        - default
      environment:
        # Enable trace logging for debugging UDP/streaming issues
        # Reduce webrtc_sctp noise to warn level (debug floods logs with peer_last_tsn messages)
        - RUST_LOG=moonlight_common=trace,moonlight_web=trace,streamer::video=info,webrtc_sctp=warn
        # Internal pairing PIN (shared with Wolf, secure random in production)
        - MOONLIGHT_INTERNAL_PAIRING_PIN=${MOONLIGHT_INTERNAL_PAIRING_PIN:-}
        # Moonlight credentials for API authentication (must match config.json)
        - MOONLIGHT_CREDENTIALS=${MOONLIGHT_CREDENTIALS:-helix}
        # TURN server credentials (must match config.json)
        - TURN_PASSWORD=${TURN_PASSWORD:-helix-turn-secret}
        # TURN server public IP (auto-detected if not set, or use --api-host from install.sh)
        - TURN_PUBLIC_IP=${TURN_PUBLIC_IP:-}
        # Note: TURN config is templated in config.json.template with {{TURN_PUBLIC_IP}}
        # Both moonlight-web and browser clients use the same ICE servers from that config
        #
        # IMPORTANT: config.json persists across restarts to avoid unnecessary regeneration.
        # If your public IP changes and browser input stops working, force a refresh:
        #   rm moonlight-web-config/config.json && docker compose -f docker-compose.dev.yaml restart moonlight-web
      volumes:
        - ./moonlight-web-config:/app/server:rw
      restart: unless-stopped
      depends_on:
        - wolf

    # Zed Agent Task Runner Pool (replaces gptscript runner entirely)
    # Scale with: docker-compose up --scale zed-runner=5
    # zed-runner:
    #     build:
    #         context: .
    #         dockerfile: Dockerfile.zed-agent-vnc
    #         args:
    #             - BUILD_TYPE=release
    #     ports:
    #         - "5901:5901"  # Expose VNC for direct testing
    #         # Moonlight/Sunshine ports for direct client testing
    #         #- "47989:47989"  # Moonlight HTTP/HTTPS main port
    #         #- "47990:47990"  # Sunshine web UI (HTTPS config interface)
    #         #- "47984:47984"  # Moonlight HTTPS control
    #         #- "47999:47999"  # Moonlight control port (TCP)
    #         #- "47999:47999/udp"  # Moonlight control port (UDP)
    #         #- "48010:48010"  # Moonlight RTSP (TCP)
    #         #- "48010:48010/udp"  # Moonlight RTSP (UDP)
    #         #- "47998:47998/udp"  # Moonlight video RTP stream (actual streaming port)
    #         #- "48000:48000/udp"  # Moonlight audio RTP stream (actual streaming port)
    #         #- "48100:48100"  # Moonlight video ping/discovery (TCP)
    #         #- "48100:48100/udp"  # Moonlight video ping/discovery (UDP)
    #         #- "48200:48200"  # Moonlight audio ping/discovery (TCP)
    #         #- "48200:48200/udp"  # Moonlight audio ping/discovery (UDP)
    #     environment:
    #         # Control plane connection
    #         - API_HOST=http://api:8080
    #         - API_TOKEN=${RUNNER_TOKEN-oh-hallo-insecure-token}
    #         - LOG_LEVEL=debug

    #         # Zed runner configuration - one session per container, then exit
    #         - CONCURRENCY=1
    #         - MAX_TASKS=0  # 0 means unlimited tasks - stay alive for multiple requests
    #         - SESSION_TIMEOUT=3600

    #         # Container uses fixed settings since each container is isolated
    #         - WORKSPACE_DIR=/tmp/workspace
    #         - WAYLAND_DISPLAY=wayland-0
    #         - WLR_RENDERER=gles2
    #         - WLR_BACKENDS=headless
    #         - WLR_HEADLESS_OUTPUTS=1
    #         - WLR_NO_HARDWARE_CURSORS=1
    #         - VNC_PASSWORD=${VNC_PASSWORD:-helix123}
    #         # Also set RDP_PASSWORD for Helix agent (backwards compatibility)
    #         - RDP_PASSWORD=${VNC_PASSWORD:-helix123}

    #         # HyprMoon screencopy configuration
    #         - HYPRMOON_FRAME_SOURCE=screencopy
    #         - HYPRMOON_WAYLAND_DISPLAY=wayland-1
    #         - HYPRMOON_DEBUG_SAVE_FRAMES=1

    #         # Wolf-style GPU configuration for container runtime
    #         - NVIDIA_VISIBLE_DEVICES=all
    #         - NVIDIA_DRIVER_CAPABILITIES=all
    #         - WOLF_RENDER_NODE=/dev/dri/renderD128
    #         - GST_GL_DRM_DEVICE=/dev/dri/renderD128
    #     volumes:
    #         # Mount source code for hot reloading during development
    #         - .:/workspace/helix
    #         # Persistence for user data in zed environment
    #         - zed-work:/home/ubuntu/work
    #         # Go caches for faster development
    #         - go-pkg-mod:/go/pkg/mod
    #         - go-build-cache:/root/.cache/go-build
    #         # Mount Zed launcher scripts for easy iteration
    #         - ./zed.sh:/usr/local/bin/zed.sh
    #         - ./zed-vgl.sh:/usr/local/bin/zed-vgl.sh
    #         - ./zed-wayland.sh:/usr/local/bin/zed-wayland.sh
    #         # Mount zed-build directory for live binary updates
    #         - ./zed-build:/zed-build
    #         # Mount startup script for fast iteration without rebuilds
    #         - ./start-wayland-vnc.sh:/start-wayland-vnc.sh
    #         - ./start-ubuntu-session.sh:/start-ubuntu-session.sh
    #         # Mount Hyprland config for fast iteration
    #         - ./hypr-config:/home/ubuntu/.config/hypr
    #         # Mount AGS config from dots-hyprland for fast iteration
    #         - /home/luke/pm/dots-hyprland/.config/ags:/home/ubuntu/.config/ags
    #         # Mount screencopy frame captures for debugging/monitoring
    #         - ./screencopy-frames:/tmp/hyprmoon_frame_dumps
    #         # Mount Zed config for persistence
    #         - ./zed-config:/home/ubuntu/.config/zed
    #     # Add comprehensive GPU access via DRI and NVIDIA devices (Wolf-style)
    #     devices:
    #         - "/dev/dri:/dev/dri"
    #         - "/dev/nvidia0:/dev/nvidia0"
    #         - "/dev/nvidiactl:/dev/nvidiactl"
    #         - "/dev/nvidia-modeset:/dev/nvidia-modeset"
    #         - "/dev/nvidia-uvm:/dev/nvidia-uvm"
    #         - "/dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools"
    #     # Enable privileged mode for better GPU access
    #     privileged: true
    #     # Explicitly use nvidia runtime for full GPU support
    #     runtime: nvidia
    #     # No port mapping needed - RDP proxied over WebSocket connection
    #     restart: always
    #     depends_on:
    #         - api
    #     deploy:
    #         # Default to 1 runner, scale with --scale zed-runner=N
    #         replicas: 1
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities: [gpu]

    # GPU-enabled runner (for LLM inference)
    runner_gpu:
        <<: *runner-config
        profiles: ["runner_gpu"]
        entrypoint: bash -c "cd /workspace/helix && export PATH=/root/go/bin:\$PATH && export HELIX_COMMAND=runner && air --build.bin /helix-runner --build.cmd 'CGO_ENABLED=1 go build -buildvcs=false -tags \"!rocm\" -ldflags \"-s -w -X github.com/helixml/helix/api/pkg/data.Version=v0.0.0+dev\" -o /helix-runner ./runner-cmd/helix-runner' --build.stop_on_error true --log.main_only true"
        build:
            context: .
            dockerfile: Dockerfile.runner
            args:
                TAG: 2025-08-13c-large
        environment:
            # Runner configuration
            - API_HOST=http://172.17.0.1:8080
            - API_TOKEN=oh-hallo-insecure-token
            - RUNNER_ID=dev-gpu-runner
            - LOG_LEVEL=debug
            - HELIX_COMMAND=runner
            # Prevent toolchain downloads
            - GOTOOLCHAIN=local
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities: [gpu]

    demos:
        profiles: ["demos"]
        build:
            context: .
            dockerfile: Dockerfile.demos
        ports:
            - ${DEMOS_PORT:-8085}:8085
        restart: always
        env_file:
            - .env
        environment:
            - PORT=8085
        entrypoint: ${DEMOS_ENTRYPOINT:-tail -f /dev/null}
        volumes:
            - ./go.mod:/app/go.mod
            - ./go.sum:/app/go.sum
            - ./demos:/app/demos

    frontend:
        # ports:
        #   - 8081:${FRONTEND_INTERNAL_PORT:-8081}
        build:
            context: .
            dockerfile: Dockerfile
            target: ui-dev-env
        restart: always
        volumes:
            - ./frontend/package.json:/app/package.json
            - ./frontend/src:/app/src
            - ./frontend/assets:/app/assets
            - ./frontend/index.html:/app/index.html
            - ./frontend/tsconfig.json:/app/tsconfig.json
            - ./frontend/vite.config.ts:/app/vite.config.ts

    haystack:
        build:
            context: ./haystack_service
        # ports:
        #   - 8001:8000
        restart: always
        environment:
            - PGVECTOR_DSN=postgresql://postgres:postgres@pgvector:5432/postgres
            - LOG_LEVEL=INFO
            - VLLM_BASE_URL=${RAG_HAYSTACK_EMBEDDINGS_BASE_URL:-} # Need to set this to an external VLLM server in development
            - VLLM_API_KEY=${RAG_HAYSTACK_EMBEDDINGS_API_KEY:-EMPTY}
            - RAG_HAYSTACK_EMBEDDINGS_MODEL=${RAG_HAYSTACK_EMBEDDINGS_MODEL:-MrLight/dse-qwen2-2b-mrl-v1}
            - RAG_HAYSTACK_EMBEDDINGS_DIM=${RAG_HAYSTACK_EMBEDDINGS_DIM:-1536}
            - RAG_HAYSTACK_EMBEDDINGS_MAX_TOKENS=${RAG_HAYSTACK_EMBEDDINGS_MAX_TOKENS:-32768}
            - RAG_HAYSTACK_CHUNK_SIZE=${RAG_HAYSTACK_CHUNK_SIZE:-500}
            - RAG_HAYSTACK_CHUNK_OVERLAP=${RAG_HAYSTACK_CHUNK_OVERLAP:-50}
            # Socket configuration for api communication
            - HELIX_EMBEDDINGS_SOCKET=/socket/embeddings.sock
            - RAG_VISION_EMBEDDINGS_SOCKET=/socket/embeddings.sock
            # Vision RAG Settings
            - RAG_VISION_ENABLED=${RAG_VISION_ENABLED:-true}
            - RAG_VISION_BASE_URL=${RAG_VISION_BASE_URL:-}
            - RAG_VISION_API_KEY=${RAG_VISION_API_KEY:-}
            - RAG_VISION_EMBEDDINGS_MODEL=${RAG_VISION_EMBEDDINGS_MODEL:-MrLight/dse-qwen2-2b-mrl-v1}
            - RAG_VISION_EMBEDDINGS_DIM=${RAG_VISION_EMBEDDINGS_DIM:-1536}
            - RAG_VISION_PGVECTOR_TABLE=${RAG_VISION_PGVECTOR_TABLE:-haystack_documents_vision}
        volumes:
            - ./haystack_service/app:/app/app
            - ./haystack_service/main.py:/app/main.py
            - helix-socket:/socket
        command:
            [
                "uvicorn",
                "main:app",
                "--host",
                "0.0.0.0",
                "--port",
                "8000",
                "--reload",
            ]
        depends_on:
            - pgvector
        extra_hosts:
            - "host.docker.internal:host-gateway"

volumes:
    helix-keycloak-db:
    helix-postgres-db:
    helix-pgvector-db:
    helix-filestore:
    helix-typesense-db:
    helix-socket:
    wolf-socket:
    zed-work:

    go-pkg-mod: # Go module cache
    go-build-cache: # Go build cache
    go-sdk-cache: # Go toolchain/SDK cache
    # No persistent volumes for zed workspaces - using tmpfs for clean sessions

networks:
    default:
        name: helix_default
        ipam:
            config:
                - subnet: 172.19.0.0/16
